{"cells":[{"cell_type":"markdown","id":"5426eafa","metadata":{},"source":["# 0. Libraries and importing the data"]},{"cell_type":"code","execution_count":3,"id":"495b7128","metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","from sklearn.model_selection import cross_validate\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","import itertools\n","import math\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","from group_lasso import GroupLasso\n","from sklearn.impute import KNNImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import Lasso\n","from sklearn.linear_model import Ridge"]},{"cell_type":"markdown","id":"e8327963","metadata":{},"source":["## Preprocessing version 1"]},{"cell_type":"markdown","id":"e4aff239","metadata":{},"source":["We import the data from the final EDA dataset:"]},{"cell_type":"code","execution_count":8,"id":"bb6db920","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>is_furnished</th>\n","      <th>has_pool</th>\n","      <th>num_crimes</th>\n","      <th>has_ac</th>\n","      <th>accepts_pets</th>\n","      <th>...</th>\n","      <th>floor_1</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1562</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>133.0</td>\n","      <td>1960.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>4.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5934</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>77.0</td>\n","      <td>1987.0</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3509</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>59.0</td>\n","      <td>1969.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5867</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>101.0</td>\n","      <td>2014.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>9.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>169</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>115.0</td>\n","      <td>2018.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4619</th>\n","      <td>4427</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>125.0</td>\n","      <td>1996.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>4.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4620</th>\n","      <td>467</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>157.0</td>\n","      <td>1992.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4621</th>\n","      <td>6266</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>62.0</td>\n","      <td>1979.0</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4622</th>\n","      <td>5391</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>89.0</td>\n","      <td>2015.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4623</th>\n","      <td>861</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>167.0</td>\n","      <td>1977.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4624 rows × 35 columns</p>\n","</div>"],"text/plain":["        id  num_rooms  num_baths  square_meters  year_built  is_furnished  \\\n","0     1562        4.0        1.0          133.0      1960.0         False   \n","1     5934        4.0        2.0           77.0      1987.0          True   \n","2     3509        1.0        1.0           59.0      1969.0          True   \n","3     5867        3.0        3.0          101.0      2014.0         False   \n","4      169        3.0        2.0          115.0      2018.0         False   \n","...    ...        ...        ...            ...         ...           ...   \n","4619  4427        3.0        2.0          125.0      1996.0         False   \n","4620   467        4.0        1.0          157.0      1992.0          True   \n","4621  6266        3.0        3.0           62.0      1979.0          True   \n","4622  5391        4.0        1.0           89.0      2015.0          True   \n","4623   861        4.0        2.0          167.0      1977.0         False   \n","\n","      has_pool  num_crimes  has_ac  accepts_pets  ...  floor_1  floor_10  \\\n","0        False         4.0   False         False  ...    False     False   \n","1         True         0.0   False         False  ...     True     False   \n","2        False         0.0   False         False  ...    False     False   \n","3        False         9.0   False          True  ...    False     False   \n","4        False         0.0   False          True  ...    False     False   \n","...        ...         ...     ...           ...  ...      ...       ...   \n","4619     False         4.0    True         False  ...    False      True   \n","4620     False         0.0   False          True  ...    False     False   \n","4621      True         0.0    True          True  ...    False     False   \n","4622     False         0.0    True         False  ...    False     False   \n","4623      True         0.0    True         False  ...    False     False   \n","\n","      floor_2  floor_3  floor_4  floor_5  floor_6  floor_7  floor_8  floor_9  \n","0       False    False    False    False    False     True    False    False  \n","1       False    False    False    False    False    False    False    False  \n","2       False    False    False     True    False    False    False    False  \n","3       False    False    False    False    False     True    False    False  \n","4        True    False    False    False    False    False    False    False  \n","...       ...      ...      ...      ...      ...      ...      ...      ...  \n","4619    False    False    False    False    False    False    False    False  \n","4620    False    False    False    False     True    False    False    False  \n","4621     True    False    False    False    False    False    False    False  \n","4622    False    False    False    False    False    False    False     True  \n","4623    False    False    False    False    False    False     True    False  \n","\n","[4624 rows x 35 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df_v1 = pd.read_csv('2_preprocessed_datasets/v1_prepr_ohe-listwise_deletion-no_num_supm-with_num_rooms_outliers.csv')\n","df_v1"]},{"cell_type":"code","execution_count":9,"id":"ad64511a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4624 entries, 0 to 4623\n","Data columns (total 35 columns):\n"," #   Column                            Non-Null Count  Dtype  \n","---  ------                            --------------  -----  \n"," 0   id                                4624 non-null   int64  \n"," 1   num_rooms                         4624 non-null   float64\n"," 2   num_baths                         4624 non-null   float64\n"," 3   square_meters                     4624 non-null   float64\n"," 4   year_built                        4624 non-null   float64\n"," 5   is_furnished                      4624 non-null   bool   \n"," 6   has_pool                          4624 non-null   bool   \n"," 7   num_crimes                        4624 non-null   float64\n"," 8   has_ac                            4624 non-null   bool   \n"," 9   accepts_pets                      4624 non-null   bool   \n"," 10  price                             4624 non-null   int64  \n"," 11  orientation_east                  4624 non-null   bool   \n"," 12  orientation_north                 4624 non-null   bool   \n"," 13  orientation_south                 4624 non-null   bool   \n"," 14  orientation_west                  4624 non-null   bool   \n"," 15  neighborhood_Ciutat Vella         4624 non-null   bool   \n"," 16  neighborhood_Eixample             4624 non-null   bool   \n"," 17  neighborhood_Gràcia               4624 non-null   bool   \n"," 18  neighborhood_Horta                4624 non-null   bool   \n"," 19  neighborhood_Les Cors             4624 non-null   bool   \n"," 20  neighborhood_Nou Barris           4624 non-null   bool   \n"," 21  neighborhood_Sant Andreu          4624 non-null   bool   \n"," 22  neighborhood_Sant Martí           4624 non-null   bool   \n"," 23  neighborhood_Sants                4624 non-null   bool   \n"," 24  neighborhood_Sarrià-Sant Gervasi  4624 non-null   bool   \n"," 25  floor_1                           4624 non-null   bool   \n"," 26  floor_10                          4624 non-null   bool   \n"," 27  floor_2                           4624 non-null   bool   \n"," 28  floor_3                           4624 non-null   bool   \n"," 29  floor_4                           4624 non-null   bool   \n"," 30  floor_5                           4624 non-null   bool   \n"," 31  floor_6                           4624 non-null   bool   \n"," 32  floor_7                           4624 non-null   bool   \n"," 33  floor_8                           4624 non-null   bool   \n"," 34  floor_9                           4624 non-null   bool   \n","dtypes: bool(28), float64(5), int64(2)\n","memory usage: 379.4 KB\n"]}],"source":["df_v1.info()"]},{"cell_type":"markdown","id":"c0a6de03","metadata":{},"source":["## Preprocessing version 2"]},{"cell_type":"code","execution_count":10,"id":"4a63b5d4","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>is_furnished</th>\n","      <th>has_pool</th>\n","      <th>num_crimes</th>\n","      <th>has_ac</th>\n","      <th>accepts_pets</th>\n","      <th>...</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","      <th>floor_nan</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9255</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>1956.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>2.0</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1562</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>133.0</td>\n","      <td>1960.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>4.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1671</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>137.0</td>\n","      <td>2000.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6088</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>41.0</td>\n","      <td>2002.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>5.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6670</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>70.0</td>\n","      <td>1979.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7995</th>\n","      <td>5735</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>95.0</td>\n","      <td>2021.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7996</th>\n","      <td>5192</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>166.0</td>\n","      <td>1995.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>9.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7997</th>\n","      <td>5391</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>89.0</td>\n","      <td>2015.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7998</th>\n","      <td>861</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>167.0</td>\n","      <td>1977.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7999</th>\n","      <td>7271</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>95.0</td>\n","      <td>2000.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>8.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8000 rows × 38 columns</p>\n","</div>"],"text/plain":["        id  num_rooms  num_baths  square_meters  year_built is_furnished  \\\n","0     9255        1.0        1.0            NaN      1956.0        False   \n","1     1562        4.0        1.0          133.0      1960.0        False   \n","2     1671        2.0        3.0          137.0      2000.0        False   \n","3     6088        1.0        2.0           41.0      2002.0        False   \n","4     6670        2.0        1.0           70.0      1979.0         True   \n","...    ...        ...        ...            ...         ...          ...   \n","7995  5735        2.0        NaN           95.0      2021.0        False   \n","7996  5192        4.0        3.0          166.0      1995.0        False   \n","7997  5391        4.0        1.0           89.0      2015.0         True   \n","7998   861        4.0        2.0          167.0      1977.0        False   \n","7999  7271        2.0        1.0           95.0      2000.0        False   \n","\n","     has_pool  num_crimes has_ac accepts_pets  ...  floor_10  floor_2  \\\n","0       False         2.0   True         True  ...     False    False   \n","1       False         4.0  False        False  ...     False    False   \n","2        True         0.0  False        False  ...     False    False   \n","3        True         5.0  False        False  ...     False    False   \n","4       False         0.0  False         True  ...     False    False   \n","...       ...         ...    ...          ...  ...       ...      ...   \n","7995    False         0.0  False        False  ...     False    False   \n","7996    False         9.0  False        False  ...     False    False   \n","7997    False         0.0   True        False  ...     False    False   \n","7998     True         0.0   True        False  ...     False    False   \n","7999     True         8.0   True        False  ...     False    False   \n","\n","      floor_3  floor_4  floor_5  floor_6  floor_7  floor_8  floor_9  floor_nan  \n","0        True    False    False    False    False    False    False      False  \n","1       False    False    False    False     True    False    False      False  \n","2       False    False    False    False    False    False    False      False  \n","3       False    False    False     True    False    False    False      False  \n","4       False    False     True    False    False    False    False      False  \n","...       ...      ...      ...      ...      ...      ...      ...        ...  \n","7995    False    False    False    False    False    False     True      False  \n","7996    False    False    False    False    False    False     True      False  \n","7997    False    False    False    False    False    False     True      False  \n","7998    False    False    False    False    False     True    False      False  \n","7999    False    False    False    False    False    False    False      False  \n","\n","[8000 rows x 38 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df_v2 = pd.read_csv('2_preprocessed_datasets/v2_prepr_ohe_with_na_dummies-no_num_supm-with_num_rooms_outliers.csv')\n","df_v2"]},{"cell_type":"code","execution_count":11,"id":"f2c6fe36","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 8000 entries, 0 to 7999\n","Data columns (total 38 columns):\n"," #   Column                            Non-Null Count  Dtype  \n","---  ------                            --------------  -----  \n"," 0   id                                8000 non-null   int64  \n"," 1   num_rooms                         8000 non-null   float64\n"," 2   num_baths                         7840 non-null   float64\n"," 3   square_meters                     7845 non-null   float64\n"," 4   year_built                        7830 non-null   float64\n"," 5   is_furnished                      7835 non-null   object \n"," 6   has_pool                          7844 non-null   object \n"," 7   num_crimes                        7840 non-null   float64\n"," 8   has_ac                            7831 non-null   object \n"," 9   accepts_pets                      7845 non-null   object \n"," 10  price                             8000 non-null   int64  \n"," 11  orientation_east                  8000 non-null   bool   \n"," 12  orientation_north                 8000 non-null   bool   \n"," 13  orientation_south                 8000 non-null   bool   \n"," 14  orientation_west                  8000 non-null   bool   \n"," 15  orientation_nan                   8000 non-null   bool   \n"," 16  neighborhood_Ciutat Vella         8000 non-null   bool   \n"," 17  neighborhood_Eixample             8000 non-null   bool   \n"," 18  neighborhood_Gràcia               8000 non-null   bool   \n"," 19  neighborhood_Horta                8000 non-null   bool   \n"," 20  neighborhood_Les Cors             8000 non-null   bool   \n"," 21  neighborhood_Nou Barris           8000 non-null   bool   \n"," 22  neighborhood_Sant Andreu          8000 non-null   bool   \n"," 23  neighborhood_Sant Martí           8000 non-null   bool   \n"," 24  neighborhood_Sants                8000 non-null   bool   \n"," 25  neighborhood_Sarrià-Sant Gervasi  8000 non-null   bool   \n"," 26  neighborhood_nan                  8000 non-null   bool   \n"," 27  floor_1                           8000 non-null   bool   \n"," 28  floor_10                          8000 non-null   bool   \n"," 29  floor_2                           8000 non-null   bool   \n"," 30  floor_3                           8000 non-null   bool   \n"," 31  floor_4                           8000 non-null   bool   \n"," 32  floor_5                           8000 non-null   bool   \n"," 33  floor_6                           8000 non-null   bool   \n"," 34  floor_7                           8000 non-null   bool   \n"," 35  floor_8                           8000 non-null   bool   \n"," 36  floor_9                           8000 non-null   bool   \n"," 37  floor_nan                         8000 non-null   bool   \n","dtypes: bool(27), float64(5), int64(2), object(4)\n","memory usage: 898.6+ KB\n"]}],"source":["df_v2.info()"]},{"cell_type":"markdown","id":"ed063bf5","metadata":{},"source":["## Preprocessing version 3"]},{"cell_type":"code","execution_count":12,"id":"38a00fd1","metadata":{},"outputs":[],"source":["df_v3 = pd.read_csv('2_preprocessed_datasets/v3_prepr-no_ohe-no_num_supm-with_num_rooms_outliers.csv')"]},{"cell_type":"code","execution_count":13,"id":"af80e43f","metadata":{},"outputs":[],"source":["# Convert the 'floor' column to string, removing the decimal point but keeping NaN values\n","df_v3['floor'] = df_v3['floor'].apply(lambda x: str(int(x)) if pd.notnull(x) else x)"]},{"cell_type":"code","execution_count":14,"id":"2779070f","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>orientation</th>\n","      <th>year_built</th>\n","      <th>is_furnished</th>\n","      <th>has_pool</th>\n","      <th>neighborhood</th>\n","      <th>num_crimes</th>\n","      <th>has_ac</th>\n","      <th>accepts_pets</th>\n","      <th>price</th>\n","      <th>floor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9255</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1956.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Sant Martí</td>\n","      <td>2.0</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>1096</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1562</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>133.0</td>\n","      <td>west</td>\n","      <td>1960.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Sants</td>\n","      <td>4.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1396</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1671</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>137.0</td>\n","      <td>NaN</td>\n","      <td>2000.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Eixample</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1263</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6088</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>41.0</td>\n","      <td>NaN</td>\n","      <td>2002.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Sants</td>\n","      <td>5.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1290</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6670</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>70.0</td>\n","      <td>NaN</td>\n","      <td>1979.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Gràcia</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>962</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7995</th>\n","      <td>5735</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>95.0</td>\n","      <td>south</td>\n","      <td>2021.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Sant Martí</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1066</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7996</th>\n","      <td>5192</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>166.0</td>\n","      <td>NaN</td>\n","      <td>1995.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Nou Barris</td>\n","      <td>9.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1255</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7997</th>\n","      <td>5391</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>89.0</td>\n","      <td>east</td>\n","      <td>2015.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Sant Martí</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1220</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7998</th>\n","      <td>861</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>167.0</td>\n","      <td>west</td>\n","      <td>1977.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Gràcia</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1547</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>7999</th>\n","      <td>7271</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>95.0</td>\n","      <td>NaN</td>\n","      <td>2000.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Nou Barris</td>\n","      <td>8.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1133</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8000 rows × 14 columns</p>\n","</div>"],"text/plain":["        id  num_rooms  num_baths  square_meters orientation  year_built  \\\n","0     9255        1.0        1.0            NaN         NaN      1956.0   \n","1     1562        4.0        1.0          133.0        west      1960.0   \n","2     1671        2.0        3.0          137.0         NaN      2000.0   \n","3     6088        1.0        2.0           41.0         NaN      2002.0   \n","4     6670        2.0        1.0           70.0         NaN      1979.0   \n","...    ...        ...        ...            ...         ...         ...   \n","7995  5735        2.0        NaN           95.0       south      2021.0   \n","7996  5192        4.0        3.0          166.0         NaN      1995.0   \n","7997  5391        4.0        1.0           89.0        east      2015.0   \n","7998   861        4.0        2.0          167.0        west      1977.0   \n","7999  7271        2.0        1.0           95.0         NaN      2000.0   \n","\n","     is_furnished has_pool neighborhood  num_crimes has_ac accepts_pets  \\\n","0           False    False   Sant Martí         2.0   True         True   \n","1           False    False        Sants         4.0  False        False   \n","2           False     True     Eixample         0.0  False        False   \n","3           False     True        Sants         5.0  False        False   \n","4            True    False       Gràcia         0.0  False         True   \n","...           ...      ...          ...         ...    ...          ...   \n","7995        False    False   Sant Martí         0.0  False        False   \n","7996        False    False   Nou Barris         9.0  False        False   \n","7997         True    False   Sant Martí         0.0   True        False   \n","7998        False     True       Gràcia         0.0   True        False   \n","7999        False     True   Nou Barris         8.0   True        False   \n","\n","      price floor  \n","0      1096     3  \n","1      1396     7  \n","2      1263     1  \n","3      1290     6  \n","4       962     5  \n","...     ...   ...  \n","7995   1066     9  \n","7996   1255     9  \n","7997   1220     9  \n","7998   1547     8  \n","7999   1133     1  \n","\n","[8000 rows x 14 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df_v3"]},{"cell_type":"code","execution_count":15,"id":"908ef733","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["id                  0\n","num_rooms           0\n","num_baths         160\n","square_meters     155\n","orientation      2341\n","year_built        170\n","is_furnished      165\n","has_pool          156\n","neighborhood      165\n","num_crimes        160\n","has_ac            169\n","accepts_pets      155\n","price               0\n","floor             149\n","dtype: int64\n"]}],"source":["print(df_v3.isin([np.nan]).sum())"]},{"cell_type":"markdown","id":"9da2c868","metadata":{},"source":["All good for applying scikit-learn's imputers!"]},{"cell_type":"code","execution_count":16,"id":"549c3b6a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 8000 entries, 0 to 7999\n","Data columns (total 14 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   id             8000 non-null   int64  \n"," 1   num_rooms      8000 non-null   float64\n"," 2   num_baths      7840 non-null   float64\n"," 3   square_meters  7845 non-null   float64\n"," 4   orientation    5659 non-null   object \n"," 5   year_built     7830 non-null   float64\n"," 6   is_furnished   7835 non-null   object \n"," 7   has_pool       7844 non-null   object \n"," 8   neighborhood   7835 non-null   object \n"," 9   num_crimes     7840 non-null   float64\n"," 10  has_ac         7831 non-null   object \n"," 11  accepts_pets   7845 non-null   object \n"," 12  price          8000 non-null   int64  \n"," 13  floor          7851 non-null   object \n","dtypes: float64(5), int64(2), object(7)\n","memory usage: 875.1+ KB\n"]}],"source":["df_v3.info()"]},{"cell_type":"markdown","id":"7ef0baf3","metadata":{},"source":["All categorical variables are stored as `object`, while the numerical variables are stored as floats or integers."]},{"cell_type":"markdown","id":"983b7cd1","metadata":{},"source":["## Preprocessing version 4"]},{"cell_type":"code","execution_count":4,"id":"3f9e13e6","metadata":{},"outputs":[],"source":["df_v4 = pd.read_csv('2_preprocessed_datasets/v4_prepr-no_ohe-no_num_supm-no_num_rooms_outliers.csv')"]},{"cell_type":"code","execution_count":7,"id":"a8eed5bc","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>orientation</th>\n","      <th>year_built</th>\n","      <th>is_furnished</th>\n","      <th>has_pool</th>\n","      <th>neighborhood</th>\n","      <th>num_crimes</th>\n","      <th>has_ac</th>\n","      <th>accepts_pets</th>\n","      <th>price</th>\n","      <th>floor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9255</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1956.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Sant Martí</td>\n","      <td>2.0</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>1096</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1562</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>133.0</td>\n","      <td>west</td>\n","      <td>1960.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Sants</td>\n","      <td>4.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1396</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1671</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>137.0</td>\n","      <td>NaN</td>\n","      <td>2000.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Eixample</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1263</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6088</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>41.0</td>\n","      <td>NaN</td>\n","      <td>2002.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Sants</td>\n","      <td>5.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1290</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6670</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>70.0</td>\n","      <td>NaN</td>\n","      <td>1979.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Gràcia</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>962</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7995</th>\n","      <td>5735</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>95.0</td>\n","      <td>south</td>\n","      <td>2021.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Sant Martí</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1066</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7996</th>\n","      <td>5192</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>166.0</td>\n","      <td>NaN</td>\n","      <td>1995.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>Nou Barris</td>\n","      <td>9.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1255</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7997</th>\n","      <td>5391</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>89.0</td>\n","      <td>east</td>\n","      <td>2015.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Sant Martí</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1220</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7998</th>\n","      <td>861</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>167.0</td>\n","      <td>west</td>\n","      <td>1977.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Gràcia</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1547</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>7999</th>\n","      <td>7271</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>95.0</td>\n","      <td>NaN</td>\n","      <td>2000.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Nou Barris</td>\n","      <td>8.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1133</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8000 rows × 14 columns</p>\n","</div>"],"text/plain":["        id  num_rooms  num_baths  square_meters orientation  year_built  \\\n","0     9255        1.0        1.0            NaN         NaN      1956.0   \n","1     1562        4.0        1.0          133.0        west      1960.0   \n","2     1671        2.0        3.0          137.0         NaN      2000.0   \n","3     6088        1.0        2.0           41.0         NaN      2002.0   \n","4     6670        2.0        1.0           70.0         NaN      1979.0   \n","...    ...        ...        ...            ...         ...         ...   \n","7995  5735        2.0        NaN           95.0       south      2021.0   \n","7996  5192        4.0        3.0          166.0         NaN      1995.0   \n","7997  5391        4.0        1.0           89.0        east      2015.0   \n","7998   861        4.0        2.0          167.0        west      1977.0   \n","7999  7271        2.0        1.0           95.0         NaN      2000.0   \n","\n","     is_furnished has_pool neighborhood  num_crimes has_ac accepts_pets  \\\n","0           False    False   Sant Martí         2.0   True         True   \n","1           False    False        Sants         4.0  False        False   \n","2           False     True     Eixample         0.0  False        False   \n","3           False     True        Sants         5.0  False        False   \n","4            True    False       Gràcia         0.0  False         True   \n","...           ...      ...          ...         ...    ...          ...   \n","7995        False    False   Sant Martí         0.0  False        False   \n","7996        False    False   Nou Barris         9.0  False        False   \n","7997         True    False   Sant Martí         0.0   True        False   \n","7998        False     True       Gràcia         0.0   True        False   \n","7999        False     True   Nou Barris         8.0   True        False   \n","\n","      price floor  \n","0      1096     3  \n","1      1396     7  \n","2      1263     1  \n","3      1290     6  \n","4       962     5  \n","...     ...   ...  \n","7995   1066     9  \n","7996   1255     9  \n","7997   1220     9  \n","7998   1547     8  \n","7999   1133     1  \n","\n","[8000 rows x 14 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Convert the 'floor' column to string, removing the decimal point but keeping NaN values\n","df_v4['floor'] = df_v4['floor'].apply(lambda x: str(int(x)) if pd.notnull(x) else x)\n","df_v4"]},{"cell_type":"code","execution_count":6,"id":"316dd6db","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 8000 entries, 0 to 7999\n","Data columns (total 14 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   id             8000 non-null   int64  \n"," 1   num_rooms      8000 non-null   float64\n"," 2   num_baths      7840 non-null   float64\n"," 3   square_meters  7845 non-null   float64\n"," 4   orientation    5659 non-null   object \n"," 5   year_built     7830 non-null   float64\n"," 6   is_furnished   7835 non-null   object \n"," 7   has_pool       7844 non-null   object \n"," 8   neighborhood   7835 non-null   object \n"," 9   num_crimes     7840 non-null   float64\n"," 10  has_ac         7831 non-null   object \n"," 11  accepts_pets   7845 non-null   object \n"," 12  price          8000 non-null   int64  \n"," 13  floor          7851 non-null   object \n","dtypes: float64(5), int64(2), object(7)\n","memory usage: 875.1+ KB\n"]}],"source":["df_v4.info()"]},{"cell_type":"markdown","id":"0eac9b2a","metadata":{},"source":["## Utilities"]},{"cell_type":"markdown","id":"cd859520","metadata":{},"source":["### For preprocessing v1, v2"]},{"cell_type":"code","execution_count":17,"id":"ee9dc318","metadata":{},"outputs":[{"data":{"text/plain":["['num_rooms',\n"," 'num_baths',\n"," 'square_meters',\n"," 'year_built',\n"," 'num_crimes',\n"," 'is_furnished',\n"," 'has_pool',\n"," 'has_ac',\n"," 'accepts_pets',\n"," ['orientation_east',\n","  'orientation_north',\n","  'orientation_south',\n","  'orientation_west'],\n"," ['neighborhood_Ciutat Vella',\n","  'neighborhood_Eixample',\n","  'neighborhood_Gràcia',\n","  'neighborhood_Horta',\n","  'neighborhood_Les Cors',\n","  'neighborhood_Nou Barris',\n","  'neighborhood_Sant Andreu',\n","  'neighborhood_Sant Martí',\n","  'neighborhood_Sants',\n","  'neighborhood_Sarrià-Sant Gervasi'],\n"," ['floor_1',\n","  'floor_2',\n","  'floor_3',\n","  'floor_4',\n","  'floor_5',\n","  'floor_6',\n","  'floor_7',\n","  'floor_8',\n","  'floor_9',\n","  'floor_10']]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Lists to store the features\n","floor_dummies = [f'floor_{str(x)}' for x in range(1, 11)]\n","neighborhood_dummies = ['neighborhood_Ciutat Vella', 'neighborhood_Eixample', 'neighborhood_Gràcia', 'neighborhood_Horta', \n","                        'neighborhood_Les Cors', 'neighborhood_Nou Barris',\n","                        'neighborhood_Sant Andreu', 'neighborhood_Sant Martí', \n","                        'neighborhood_Sants', 'neighborhood_Sarrià-Sant Gervasi']\n","orientation_dummies = ['orientation_east', 'orientation_north', 'orientation_south', 'orientation_west']\n","\n","categorical_features = ['is_furnished', 'has_pool', 'has_ac','accepts_pets', \n","                            orientation_dummies, neighborhood_dummies,floor_dummies]\n","\n","numerical_features = ['num_rooms',\n"," 'num_baths',\n"," 'square_meters',\n"," 'year_built',\n"," 'num_crimes']\n","\n","features = numerical_features + categorical_features\n","features"]},{"cell_type":"markdown","id":"1eaa04a4","metadata":{},"source":["Features with k-1 dummies:"]},{"cell_type":"code","execution_count":18,"id":"e1e4c581","metadata":{},"outputs":[{"data":{"text/plain":["['num_rooms',\n"," 'num_baths',\n"," 'square_meters',\n"," 'year_built',\n"," 'num_crimes',\n"," 'is_furnished',\n"," 'has_pool',\n"," 'has_ac',\n"," 'accepts_pets',\n"," ['orientation_north', 'orientation_south', 'orientation_west'],\n"," ['neighborhood_Eixample',\n","  'neighborhood_Gràcia',\n","  'neighborhood_Horta',\n","  'neighborhood_Les Cors',\n","  'neighborhood_Nou Barris',\n","  'neighborhood_Sant Andreu',\n","  'neighborhood_Sant Martí',\n","  'neighborhood_Sants',\n","  'neighborhood_Sarrià-Sant Gervasi'],\n"," ['floor_2',\n","  'floor_3',\n","  'floor_4',\n","  'floor_5',\n","  'floor_6',\n","  'floor_7',\n","  'floor_8',\n","  'floor_9',\n","  'floor_10']]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["floor_dummies_incomplete = [f'floor_{str(x)}' for x in range(2, 11)]\n","neighborhood_dummies_incomplete = ['neighborhood_Eixample', 'neighborhood_Gràcia', 'neighborhood_Horta', \n","                        'neighborhood_Les Cors', 'neighborhood_Nou Barris',\n","                        'neighborhood_Sant Andreu', 'neighborhood_Sant Martí', \n","                        'neighborhood_Sants', 'neighborhood_Sarrià-Sant Gervasi']\n","orientation_dummies_incomplete = ['orientation_north', 'orientation_south', 'orientation_west']\n","\n","categorical_features_incomplete = ['is_furnished', 'has_pool', 'has_ac','accepts_pets', \n","                            orientation_dummies_incomplete, neighborhood_dummies_incomplete,floor_dummies_incomplete]\n","\n","features_incomplete = numerical_features + categorical_features_incomplete\n","features_incomplete"]},{"cell_type":"markdown","id":"e3cee544","metadata":{},"source":["### For preprocessing v3, v4"]},{"cell_type":"code","execution_count":8,"id":"fd74c82b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['num_rooms', 'num_baths', 'square_meters', 'year_built', 'num_crimes', 'is_furnished', 'has_pool', 'has_ac', 'accepts_pets', 'orientation', 'neighborhood', 'floor']\n"]}],"source":["categorical_cols_v3 = ['orientation', 'neighborhood', 'floor']\n","boolean_cols_v3 = ['is_furnished', 'has_pool', 'has_ac', 'accepts_pets']\n","numerical_cols_v3 = ['num_rooms', 'num_baths', 'square_meters', 'year_built', 'num_crimes']\n","\n","features_v3 = numerical_cols_v3 + boolean_cols_v3 + categorical_cols_v3\n","print(features_v3)"]},{"cell_type":"markdown","id":"2e8e7ccf","metadata":{},"source":["### Functions"]},{"cell_type":"markdown","id":"dc13889b","metadata":{},"source":["Function that flattens a list (i.e., converts a nested list into a 1-D list):"]},{"cell_type":"code","execution_count":20,"id":"575fd320","metadata":{},"outputs":[],"source":["def flatten(lst):\n","    return list(itertools.chain(*[x if isinstance(x, list) else [x] for x in lst]))"]},{"cell_type":"markdown","id":"6361e1ee","metadata":{},"source":["# 3. Feature selection and evaluation\n","- Apply wrapper methods? Forward Feature Selection, Backward Feature Elimination and/or Sequential Floating.\n","- Apply regularization techniques? i.e., LASSO, Ridge, etc. They are embedded methods, which involve building and evaluating models for different feature subsets (as well as wrapper methods), but their feature selection process happens at the same time as their model fitting step.\n","\n","Predict values in the validation dataset(s) and check the accuracy (in this case, through the MSE) of the prediction. Lower MSE indicates better predictions. Go back to some steps in 2 onwards for improving the accuracy of the model. When you want to improve your model, you can either:\n","- Create new features. This is relevant even if you have considered all of the data you had. For example, if you have the data of an observation, the model may not consider this information adequately. On the other hand, if instead you trained the model with a feature which is the day of the week, that may be relevant because of seasonality.\n","- Try different models.\n","- Ensemble models: for instance, if the random forest considers very good the trend and a decision tree the seasonality, you can combine both.\n","- Hyperparameters tuning.\n","\n","Things to do in this section:\n","1. Split the data.\n","2. Treat missing values (if there is no listwise deletion).\n","3. Treat outliers.\n","4. Standardize. Apply same standardization to validation data.\n","5. Train model for each fold, with each possible combination of regressors.\n","6. Evaluate model through MSE.\n","7. Construct dataframe with the coefficient for each variable if it has been included in the model (NaN otherwise), the fold number, the number of regressors included (counting all of the dummy variables individually) and the MSE."]},{"cell_type":"markdown","id":"92ceef70","metadata":{},"source":["## Function 1: combination of features (considering OHE variables as a unit; exclude one dummy before to prevent multicollinearity!) -> flatten list of features -> standardization -> linear regression -> cross-validation"]},{"cell_type":"code","execution_count":21,"id":"ecfeeb4c","metadata":{},"outputs":[],"source":["# Function to perform linear regressions for each subset of features, and with\n","# cross-validation\n","\n","def lin_reg_cv_v5(df: pd.DataFrame, features: list, target: str, n_folds: int = 5):\n","\n","    def flatten_combo(combo):\n","        \"\"\"\n","        Flattens the combination by including all variables from nested lists\n","        when one element of a list is selected.\n","        \"\"\"\n","        return list(itertools.chain(*[x if isinstance(x, list) else [x] for x in combo]))\n","\n","    results_list = []  # To store all results we want for our data frame\n","    combo_counter = 0  # Initialize a counter for the combinations\n","\n","    # We create a loop which will consider each possible combination of features\n","    # from 1 regressor up to the total number of regressors\n","    for r in range(1, len(features) + 1):\n","        for combo in itertools.combinations(features, r):\n","            \n","            # We add 1 to the counter each time we consider a combination\n","            combo_counter += 1\n","\n","            # We flatten the list of the resulting combination of features\n","            flat_combo = flatten_combo(combo)\n","\n","            # We create a pipeline where, first, the training data will be standardized\n","            # and, then, the same standardization will be applied to the test data.\n","            # Second, the pipeline will fit a linear regression onto the training data.\n","            pipeline = Pipeline([\n","                ('scaler', StandardScaler()),\n","                ('linreg', LinearRegression(fit_intercept = True))\n","            ])\n","\n","            # Perform cross-validation: on each fold, it will scale and fit the training\n","            # data, and compute the negative mean squared error (so a higher value, i.e.,\n","            # more negative, is better).\n","            cv_results = cross_validate(estimator=pipeline, \n","                                        X=df[flat_combo], y=df[target], cv=n_folds, \n","                                        scoring='neg_mean_squared_error',\n","                                        return_estimator=True)\n","\n","            neg_mse = cv_results['test_score']  # Negative MSE scores\n","            estimators = cv_results['estimator']  # List of fitted estimators for each fold\n","\n","            # Loop through each fold result\n","            for index, (score, estimator) in enumerate(zip(neg_mse, estimators)):\n","                linreg = estimator.named_steps['linreg']\n","                coefs = linreg.coef_\n","                intercept = linreg.intercept_\n","\n","                # Create a dictionary to store the results for this fold\n","                result_dict = {\n","                    'Fold': index + 1,\n","                    'Combination ID': combo_counter,\n","                    'Negative MSE': score,\n","                    'Intercept': intercept\n","                }\n","\n","                # Assign coefficients to the corresponding features\n","                for i, feature in enumerate(flat_combo):\n","                    result_dict[feature] = coefs[i]\n","\n","                # For features not in the current combination, assign NaN\n","                for feature_group in features:\n","                    # Flatten the feature group (handle nested lists)\n","                    feature_list = flatten_combo([feature_group])\n","                    \n","                    # If the feature group was not selected, assign NaN to all its members\n","                    if not any(f in flat_combo for f in feature_list):\n","                        for feature in feature_list:\n","                            result_dict[feature] = float('nan')\n","\n","                # Append the result to the list\n","                results_list.append(result_dict)\n","\n","    # Convert the results list to a DataFrame\n","    results_df = pd.DataFrame(results_list)\n","\n","    return results_df"]},{"cell_type":"markdown","id":"c26c2203","metadata":{},"source":["References:\n","- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n","- https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\n","- https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n","- https://www.geeksforgeeks.org/what-is-exactly-sklearnpipelinepipeline/\n","- https://docs.python.org/3/library/itertools.html#itertools.combinations"]},{"cell_type":"markdown","id":"4ce7573b","metadata":{},"source":["### Combinations"]},{"cell_type":"markdown","id":"ce41a07b","metadata":{},"source":["Number of combinations in this function for 12 features (considering each group of dummies that come from one variable as a feature):"]},{"cell_type":"code","execution_count":22,"id":"cb3c584c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4095\n"]}],"source":["combo_list = []\n","\n","for r in range(1, len(features) + 1):\n","    for combo in itertools.combinations(features, r):\n","        combo_list.append(list(combo))\n","\n","print(len(combo_list))"]},{"cell_type":"markdown","id":"1556e8a4","metadata":{},"source":["Which matches to the sum of combinations of assigning 12 regressors to k groups, from k = 1 to k = 12!"]},{"cell_type":"code","execution_count":23,"id":"b9950e7f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4095\n"]}],"source":["comb = 0\n","\n","for k in range(1, 12+1):\n","    num_comb = math.comb(12, k)\n","    comb += num_comb\n","\n","print(comb)"]},{"cell_type":"markdown","id":"cda005bc","metadata":{},"source":["So if we do 5 folds, there will be 4095 * 5 = 20475 different fitted regressions!"]},{"cell_type":"markdown","id":"e8fef89c","metadata":{},"source":["### Training with function 1"]},{"cell_type":"markdown","id":"c67e8826","metadata":{},"source":["First, we drop a dummy from each of the categorical variables that we have OHE, in order to prevent multicollinearity:"]},{"cell_type":"code","execution_count":24,"id":"f6e8afde","metadata":{},"outputs":[{"data":{"text/plain":["['num_rooms',\n"," 'num_baths',\n"," 'square_meters',\n"," 'year_built',\n"," 'num_crimes',\n"," 'is_furnished',\n"," 'has_pool',\n"," 'has_ac',\n"," 'accepts_pets',\n"," ['orientation_north', 'orientation_south', 'orientation_west'],\n"," ['neighborhood_Eixample',\n","  'neighborhood_Gràcia',\n","  'neighborhood_Horta',\n","  'neighborhood_Les Cors',\n","  'neighborhood_Nou Barris',\n","  'neighborhood_Sant Andreu',\n","  'neighborhood_Sant Martí',\n","  'neighborhood_Sants',\n","  'neighborhood_Sarrià-Sant Gervasi'],\n"," ['floor_2',\n","  'floor_3',\n","  'floor_4',\n","  'floor_5',\n","  'floor_6',\n","  'floor_7',\n","  'floor_8',\n","  'floor_9',\n","  'floor_10']]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["floor_dummies_f1 = [f'floor_{str(x)}' for x in range(2, 11)]\n","neighborhood_dummies_f1 = ['neighborhood_Eixample', 'neighborhood_Gràcia', 'neighborhood_Horta', \n","                        'neighborhood_Les Cors', 'neighborhood_Nou Barris',\n","                        'neighborhood_Sant Andreu', 'neighborhood_Sant Martí', \n","                        'neighborhood_Sants', 'neighborhood_Sarrià-Sant Gervasi']\n","orientation_dummies_f1 = ['orientation_north', 'orientation_south', 'orientation_west']\n","\n","categorical_features_f1 = ['is_furnished', 'has_pool', 'has_ac','accepts_pets', \n","                            orientation_dummies_f1, neighborhood_dummies_f1,floor_dummies_f1]\n","\n","features_f1 = numerical_features + categorical_features_f1\n","features_f1"]},{"cell_type":"markdown","id":"dc16fe3c","metadata":{},"source":["Now, we apply the function to dataset v1:"]},{"cell_type":"code","execution_count":25,"id":"68c5a5d1","metadata":{},"outputs":[],"source":["# results_v1_f1 = lin_reg_cv_v5(df = df_v1, features = features_f1, target = 'price', n_folds = 5)\n","# results_v1_f1.to_csv('3_results_datasets/results_v1_f1.csv')"]},{"cell_type":"markdown","id":"d8a1856c","metadata":{},"source":["Approximately 1 minute training."]},{"cell_type":"code","execution_count":26,"id":"5aecb277","metadata":{},"outputs":[],"source":["# results_v1_f1.head()"]},{"cell_type":"markdown","id":"fa7da58f","metadata":{},"source":["## Function 2: flatten -> combination of features (automatic exclusion of 1 dummy) -> standardization -> linear regression -> cross-validation\n","***Function not fine-tuned because it is not computationally feasible to compute all of the possibilities (more than 8 billion combinations)!***"]},{"cell_type":"markdown","id":"6a457205","metadata":{},"source":["### Combinations"]},{"cell_type":"code","execution_count":27,"id":"4986610f","metadata":{},"outputs":[{"data":{"text/plain":["33"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["len(flatten(features))"]},{"cell_type":"code","execution_count":28,"id":"bf26801e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["8589934591\n"]}],"source":["# Since there are 33 features overall, the total number of combinations is approx.\n","# given by:\n","\n","comb = 0\n","\n","for k in range(1, len(flatten(features))+1):\n","    num_comb = math.comb(len(flatten(features)), k)\n","    comb += num_comb\n","\n","print(comb)"]},{"cell_type":"markdown","id":"5710a1ef","metadata":{},"source":["This is probably NOT computationally feasible (more than 8 billion combinations)!!!"]},{"cell_type":"markdown","id":"a43c2ccf","metadata":{},"source":["## Function 3: features (considering OHE variables as a unit; exclude one dummy before to avoid multicollinearity!) -> flatten list of features -> standardization -> Group LASSO (with alpha parameter tuning) -> cross-validation\n","Remember that Lasso tends to remove less features than Ridge. "]},{"cell_type":"markdown","metadata":{},"source":["### Treatment of dummies that come from OHE (ChatGPT)\n","\n","When dealing with one-hot-encoded categorical variables in Lasso or Ridge, it's important to ensure that the regularization treats each categorical variable as a unit, rather than treating each dummy variable (each column in the one-hot encoding) independently. Otherwise, regularization could shrink or eliminate some dummy variables while keeping others, which can lead to nonsensical results.\n","\n","Here’s how to handle this situation:\n","\n","1. **Group Regularization with Lasso/Ridge:**\n","   - Standard Lasso and Ridge penalize coefficients individually, so if you directly apply them to one-hot-encoded dummy variables, you risk having some dummy variables retained while others are shrunk to zero. This breaks the categorical variable's integrity.\n","   - To address this, you can use a **grouped version of regularization**, such as:\n","     - **Group Lasso**: Ensures that either all dummy variables corresponding to a categorical variable are retained or none of them are.\n","     - **Group Ridge**: Similarly, treats all dummy variables of the same category as a single group when regularizing.\n","\n","   These methods ensure that regularization applies to the entire set of dummies associated with a categorical variable, keeping the structure of the original categorical variable intact.\n","\n","2. **Manual Feature Handling (Standard Lasso/Ridge):**\n","   - If you're using standard Lasso or Ridge without the group version, you may need to manually ensure that either **all or none** of the dummies corresponding to a single categorical variable are included in the model.\n","   - One way to do this is by **preselecting features** or running regularization only on non-categorical variables, and then adding all the dummy variables of any selected categorical variable manually.\n","\n","3. **MSE Calculation**:\n","   - When evaluating the MSE during cross-validation or after fitting the model, ensure that all dummies for a categorical variable are either included or excluded as a group (if you're not using group regularization). This gives you a meaningful prediction that preserves the categorical nature.\n","\n","In your case, since you are working with feature combinations and treating one-hot-encoded categories as a single feature (from previous discussions), it would make sense to use **Group Lasso** or manually enforce the inclusion of entire dummy groups when checking the MSE.\n","\n","***On whether to drorp one dummy***:\n","\n","Yes, you should typically drop one dummy variable when incorporating one-hot-encoded categorical variables into a linear regression model, including Lasso and Ridge regularization. This prevents the problem of perfect multicollinearity, which occurs when the dummy variables are perfectly linearly dependent (i.e., their sum adds up to one).\n","Why Drop One Dummy?\n","\n","1. In a one-hot encoding, each category of a variable is represented by a binary column (dummy variable). If you include all dummies, the presence or absence of each category can be perfectly predicted from the others, leading to multicollinearity.\n","   - Dropping one dummy (usually referred to as the reference category) resolves this issue by making the remaining dummies interpretable as deviations from the reference category.\n","\n","2. How This Applies to Lasso and Ridge:\n","   - Lasso and Ridge don't explicitly handle multicollinearity the same way as unregularized linear regression. However, if you don't drop one dummy, the regularization might not fully solve the multicollinearity problem since the model could still have trouble assigning meaningful coefficients to features that are perfectly correlated.\n","   - By dropping one dummy, you avoid perfect multicollinearity and ensure that your model remains interpretable and stable, whether you're using Lasso, Ridge, or ordinary linear regression.\n","\n","3. Workflow for One-Hot Encoding with Regularization:\n","\n","   - One-Hot Encode the Categorical Variables.\n","   - Drop one dummy from each categorical variable to avoid multicollinearity.\n","   - Apply Lasso or Ridge to the resulting dataset, ensuring that categorical variables are treated as a unit (using Group Lasso/Ridge or manually ensuring inclusion/exclusion of all dummies for a given category).\n","   - Check for MSE and proceed with selecting the best features.\n","\n","By dropping one dummy, you also maintain interpretability because the remaining coefficients for the dummies can be interpreted relative to the reference category.\n","\n","References:\n","- https://stats.stackexchange.com/questions/214325/why-use-group-lasso-instead-of-lasso"]},{"cell_type":"markdown","id":"d45cec22","metadata":{},"source":["### Steps to implement Group Lasso (ChatGPT)"]},{"cell_type":"markdown","id":"ed3e55eb","metadata":{},"source":["To implement **Group Lasso** in Python, you can use the `GroupLasso` class from the `group-lasso` package, which extends the Lasso regularization to treat groups of features (such as one-hot-encoded dummy variables for categorical features) as a unit. Here’s how you can do it:\n","\n","1. **Install the `group-lasso` package:**\n","   ```bash\n","   pip install group-lasso\n","   ```\n","\n","2. **Prepare your data:**\n","   - You need to group the features, which means creating an array that indicates which features belong to the same group (for example, all dummy variables from the same categorical feature would belong to one group).\n","\n","3. **Define and apply the Group Lasso model:**\n","\n","Here's a simple example showing how to implement **Group Lasso** for a dataset with one-hot-encoded categorical variables."]},{"cell_type":"code","execution_count":29,"id":"63f2d572","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Squared Error: 3012451088.6857295\n","Coefficients: [[ 42881.9392877 ]\n"," [ 60469.30799274]\n"," [  8070.40757293]\n"," [ -8070.40757293]\n"," [ -5453.00239856]\n"," [ 28470.30292609]\n"," [-22136.49419348]]\n"]}],"source":["# Example data\n","X = pd.DataFrame({\n","    'age': [25, 32, 47, 51, 62, 23, 36, 44, 52, 48],\n","    'income': [50000, 60000, 120000, 100000, 90000, 40000, 75000, 110000, 85000, 95000],\n","    'gender_male': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n","    'gender_female': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n","    'region_A': [1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n","    'region_B': [0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n","    'region_C': [0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n","})\n","\n","y = np.array([300000, 350000, 500000, 450000, 600000, 200000, 400000, 550000, 500000, 480000])\n","\n","# Group array, defining which features are part of the same group\n","# 'age' and 'income' are in separate groups, 'gender_*' is one group, and 'region_*' is another group\n","groups = [1, 2, 3, 3, 4, 4, 4]  # Group '1' for 'age', '2' for 'income', '3' for 'gender', '4' for 'region'\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Define the Group Lasso model\n","group_lasso = GroupLasso(\n","    groups=groups,  # Grouping array\n","    group_reg=0.1,  # Regularization strength for groups (default is 0.05)\n","    l1_reg=0.1,     # Lasso regularization hyperparameter (alpha)\n","    fit_intercept=True,\n","    scale_reg=\"group_size\",  # Scales penalty by group size (optional)\n","    supress_warning = True\n",")\n","\n","# Fit the model\n","group_lasso.fit(X_train_scaled, y_train)\n","\n","# Predict and calculate MSE\n","y_pred = group_lasso.predict(X_test_scaled)\n","mse = mean_squared_error(y_test, y_pred)\n","\n","print(f'Mean Squared Error: {mse}')\n","print(f'Coefficients: {group_lasso.coef_}')"]},{"cell_type":"markdown","id":"1954858c","metadata":{},"source":["Explanation:\n","\n","1. **Data**: \n","   - `X` is a dataframe containing numerical and one-hot-encoded categorical variables.\n","   - `groups` specifies how features are grouped. In this example:\n","     - `age` and `income` are in their own groups.\n","     - `gender_male` and `gender_female` are in group 3.\n","     - `region_A`, `region_B`, and `region_C` are in group 4.\n","\n","2. **Scaling**: The features are standardized before applying Group Lasso since regularization techniques like Lasso and Ridge work better when features are on the same scale.\n","\n","3. **Group Lasso Model**:\n","   - `group_reg` controls the regularization strength for the groups (larger values shrink entire groups of features).\n","   - `l1_reg` controls the Lasso penalty.\n","   - `groups` defines the group structure, ensuring categorical variables are treated as a unit.\n","\n","4. **Output**:\n","   - The model returns the fitted coefficients, and you can calculate the MSE on the test data.\n","\n","Notes:\n","- You should adjust the regularization strengths (`group_reg` and `l1_reg`) based on your dataset and desired level of feature selection.\n","- If you want to treat one-hot-encoded categorical variables as a unit, the group structure should include all dummy variables for a category in the same group.\n","\n","This method ensures that all dummy variables for a categorical feature are either kept together or removed as a whole during the regularization process."]},{"cell_type":"markdown","id":"71f3a864","metadata":{},"source":["References:\n","- https://group-lasso.readthedocs.io/en/latest/\n","- https://group-lasso.readthedocs.io/en/latest/auto_examples/example_sparse_group_lasso.html\n","- API reference of GroupLasso: https://group-lasso.readthedocs.io/en/latest/api_reference.html"]},{"cell_type":"markdown","id":"1b3a1c99","metadata":{},"source":["### Integrating `GroupLasso` in the `scikit-learn` pipeline"]},{"cell_type":"markdown","id":"51f84b00","metadata":{},"source":["Finally, it is also possible to integrate `GroupLasso` within the scikit-learn pipeline. See an example below, which yields the same results as the example above."]},{"cell_type":"code","execution_count":30,"id":"340dc864","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Squared Error: 3012412897.978672\n","Coefficients: [[ 42882.43237837]\n"," [ 60468.91167671]\n"," [  8070.38649143]\n"," [ -8070.38649143]\n"," [ -5453.14774949]\n"," [ 28470.06047625]\n"," [-22136.51700127]]\n","Intercept: [435000.86541208]\n","Coefficient number 1 : 42882.432378368525\n","Coefficient number 2 : 60468.911676710384\n","Coefficient number 3 : 8070.386491427103\n","Coefficient number 4 : -8070.386491427103\n","Coefficient number 5 : -5453.147749491385\n","Coefficient number 6 : 28470.060476253948\n","Coefficient number 7 : -22136.517001267865\n"]}],"source":["# Example dataset\n","X = pd.DataFrame({\n","    'age': [25, 32, 47, 51, 62, 23, 36, 44, 52, 48],\n","    'income': [50000, 60000, 120000, 100000, 90000, 40000, 75000, 110000, 85000, 95000],\n","    'gender_male': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n","    'gender_female': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n","    'region_A': [1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n","    'region_B': [0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n","    'region_C': [0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n","})\n","\n","y = np.array([300000, 350000, 500000, 450000, 600000, 200000, 400000, 550000, 500000, 480000])\n","\n","# Group array, defining which features are part of the same group\n","# 'age' and 'income' are in separate groups, 'gender_*' is one group, and 'region_*' is another group\n","groups = [1, 2, 3, 3, 4, 4, 4]  # Group '1' for 'age', '2' for 'income', '3' for 'gender', '4' for 'region'\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a pipeline for preprocessing and modeling\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),  # Standardize features\n","    ('group_lasso', GroupLasso(groups=groups, group_reg=0.1, l1_reg=0.1, fit_intercept=True, supress_warning=True))\n","])\n","\n","# Fit the pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = pipeline.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","\n","print(f'Mean Squared Error: {mse}')\n","print(f'Coefficients: {pipeline.named_steps[\"group_lasso\"].coef_}')\n","print(f'Intercept: {pipeline.named_steps[\"group_lasso\"].intercept_}')\n","\n","for i in range(len(pipeline.named_steps[\"group_lasso\"].coef_)):\n","    print('Coefficient number', i + 1, ':', pipeline.named_steps[\"group_lasso\"].coef_[i][0])"]},{"cell_type":"markdown","id":"5cebc944","metadata":{},"source":["### Adding KNN imputer in the pipeline"]},{"cell_type":"markdown","id":"d05ccc05","metadata":{},"source":["Finally, it is also possible to integrate the KNN imputer in the pipeline, after standardization but before Group Lasso:"]},{"cell_type":"code","execution_count":31,"id":"c215d563","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Squared Error: 13663955712.572994\n","Coefficients: [[ 19539.83764285]\n"," [ 95027.61005906]\n"," [ -3700.95143226]\n"," [  3700.95143226]\n"," [ -8145.32673929]\n"," [ 44530.73101544]\n"," [-35076.07732598]]\n"]},{"name":"stderr","output_type":"stream","text":["/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n"]}],"source":["# Example dataset with missing values (NaN)\n","X = pd.DataFrame({\n","    'age': [25, 32, np.nan, 51, 62, 23, np.nan, 44, 52, 48],\n","    'income': [50000, 60000, 120000, np.nan, 90000, 40000, 75000, np.nan, 85000, 95000],\n","    'gender_male': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n","    'gender_female': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n","    'region_A': [1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n","    'region_B': [0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n","    'region_C': [0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n","})\n","\n","y = np.array([300000, 350000, 500000, 450000, 600000, 200000, 400000, 550000, 500000, 480000])\n","\n","# Group array, defining which features are part of the same group\n","groups = [1, 2, 3, 3, 4, 4, 4]  # Group '1' for 'age', '2' for 'income', '3' for 'gender', '4' for 'region'\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a pipeline for preprocessing and modeling\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),  # Standardize features\n","    ('imputer', KNNImputer(n_neighbors=3)),  # Impute missing values using KNN\n","    ('group_lasso', GroupLasso(groups=groups, group_reg=0.1, l1_reg=0.1, fit_intercept=True, scale_reg=\"group_size\", supress_warning=True))\n","])\n","\n","# Fit the pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = pipeline.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","\n","print(f'Mean Squared Error: {mse}')\n","print(f'Coefficients: {pipeline.named_steps[\"group_lasso\"].coef_}')"]},{"cell_type":"markdown","id":"0a4bcc0e","metadata":{},"source":["Key Steps:\n","1. StandardScaler: First, the features are standardized.\n","    - Standardization is done before KNN Imputation because the KNN algorithm performs better when features are on the same scale (it uses distance to find the nearest neighbors).\n","2. KNNImputer: Missing values are imputed using K-nearest neighbors based on the scaled data. The parameter n_neighbors=3 determines how many neighbors are considered for imputation.\n","3. GroupLasso: After imputation, the GroupLasso model is fitted, with the grouped features as described previously.\n","\n","Why This Order?\n","- Standardization before imputation ensures the KNN algorithm can work effectively, as distance-based imputation methods are sensitive to feature scales.\n","- Imputation after standardization fills in missing values with data that maintains the relationships between features.\n","- Finally, Group Lasso operates on the fully imputed dataset, with no missing values, enabling group-wise feature selection.\n","\n","References of `KNNImputer`:\n","- API: https://scikit-learn.org/1.5/modules/generated/sklearn.impute.KNNImputer.html\n","- https://scikit-learn.org/1.5/modules/impute.html. Key info on this imputation:\n","    - The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. \n","    - Each missing feature is imputed using values from n_neighbors nearest neighbors **that have a value for the feature**.\n","    - The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. \n","    - If a sample has more than one feature missing, then **the neighbors for that sample can be different depending on the particular feature being imputed**.\n","    - **When the number of available neighbors is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation**."]},{"cell_type":"code","execution_count":32,"id":"8114a26a","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>is_furnished</th>\n","      <th>has_pool</th>\n","      <th>num_crimes</th>\n","      <th>has_ac</th>\n","      <th>accepts_pets</th>\n","      <th>...</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","      <th>floor_nan</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9255</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>1956.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>2.0</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1562</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>133.0</td>\n","      <td>1960.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>4.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1671</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>137.0</td>\n","      <td>2000.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6088</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>41.0</td>\n","      <td>2002.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>5.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6670</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>70.0</td>\n","      <td>1979.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7995</th>\n","      <td>5735</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>95.0</td>\n","      <td>2021.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7996</th>\n","      <td>5192</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>166.0</td>\n","      <td>1995.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>9.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7997</th>\n","      <td>5391</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>89.0</td>\n","      <td>2015.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7998</th>\n","      <td>861</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>167.0</td>\n","      <td>1977.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7999</th>\n","      <td>7271</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>95.0</td>\n","      <td>2000.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>8.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8000 rows × 38 columns</p>\n","</div>"],"text/plain":["        id  num_rooms  num_baths  square_meters  year_built is_furnished  \\\n","0     9255        1.0        1.0            NaN      1956.0        False   \n","1     1562        4.0        1.0          133.0      1960.0        False   \n","2     1671        2.0        3.0          137.0      2000.0        False   \n","3     6088        1.0        2.0           41.0      2002.0        False   \n","4     6670        2.0        1.0           70.0      1979.0         True   \n","...    ...        ...        ...            ...         ...          ...   \n","7995  5735        2.0        NaN           95.0      2021.0        False   \n","7996  5192        4.0        3.0          166.0      1995.0        False   \n","7997  5391        4.0        1.0           89.0      2015.0         True   \n","7998   861        4.0        2.0          167.0      1977.0        False   \n","7999  7271        2.0        1.0           95.0      2000.0        False   \n","\n","     has_pool  num_crimes has_ac accepts_pets  ...  floor_10  floor_2  \\\n","0       False         2.0   True         True  ...     False    False   \n","1       False         4.0  False        False  ...     False    False   \n","2        True         0.0  False        False  ...     False    False   \n","3        True         5.0  False        False  ...     False    False   \n","4       False         0.0  False         True  ...     False    False   \n","...       ...         ...    ...          ...  ...       ...      ...   \n","7995    False         0.0  False        False  ...     False    False   \n","7996    False         9.0  False        False  ...     False    False   \n","7997    False         0.0   True        False  ...     False    False   \n","7998     True         0.0   True        False  ...     False    False   \n","7999     True         8.0   True        False  ...     False    False   \n","\n","      floor_3  floor_4  floor_5  floor_6  floor_7  floor_8  floor_9  floor_nan  \n","0        True    False    False    False    False    False    False      False  \n","1       False    False    False    False     True    False    False      False  \n","2       False    False    False    False    False    False    False      False  \n","3       False    False    False     True    False    False    False      False  \n","4       False    False     True    False    False    False    False      False  \n","...       ...      ...      ...      ...      ...      ...      ...        ...  \n","7995    False    False    False    False    False    False     True      False  \n","7996    False    False    False    False    False    False     True      False  \n","7997    False    False    False    False    False    False     True      False  \n","7998    False    False    False    False    False     True    False      False  \n","7999    False    False    False    False    False    False    False      False  \n","\n","[8000 rows x 38 columns]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["imputer = KNNImputer(n_neighbors = 5)\n","# Note how this is not correct as first we would need to standardize the data\n","imputer.fit_transform(df_v2)\n","df_v2"]},{"cell_type":"markdown","id":"1851cb40","metadata":{},"source":["Nothing has changed!! Note, as well, that in the case of the dummies we are handling NaNs in the dummies themselves, which complicates things. Maybe a better idea would be to label categories and, after that, apply OHE."]},{"cell_type":"markdown","id":"5cbd645d","metadata":{},"source":["### Definition of the function"]},{"cell_type":"code","execution_count":33,"id":"2838ee6e","metadata":{},"outputs":[],"source":["def group_lasso(df: pd.DataFrame, features: list, target: str, lasso_groups: list, n_folds: int = 5, max_n_lasso_iter: int = 100):\n","\n","    results_list = []  # To store all results we want for our data frame\n","    alpha_counter = 0  # Initialize a counter for the alphas\n","\n","    # We create the steps of the pipeline where, first, the training data will be standardized\n","    # and, then, the same standardization will be applied to the test data.\n","    # Second, the pipeline will impute null values through KNN, if it is desired.\n","    # Finally, it will perform Group Lasso to the imputed training and test dataset.\n","    steps = [\n","        ('scaler', StandardScaler()),  # Standardize features\n","    ]\n","    \n","    # Now, we create a first loop that iterates over different values of the alpha\n","    # parameter. We generate 50 alphas in log from -10 to 0\n","    for alpha in np.logspace(-10, 0, 50):\n","\n","        # We add Group Lasso to the pipeline, with a specific alpha\n","        steps.append(('group_lasso', \n","                    GroupLasso(\n","                        groups = lasso_groups, \n","                        group_reg = 0.05, \n","                        l1_reg = alpha, \n","                        fit_intercept = True, \n","                        n_iter = max_n_lasso_iter,\n","                        scale_reg = \"group_size\",\n","                        supress_warning = True\n","                        )))\n","\n","        # We add 1 to the counter of alpha each time we consider another alpha\n","        alpha_counter += 1\n","        \n","        # We create the pipeline with the steps specified above\n","        pipeline = Pipeline(steps)\n","\n","        # Perform cross-validation: on each fold, it will scale and fit the training\n","        # data, and compute the negative mean squared error (so a higher value, i.e.,\n","        # more negative, is better).\n","\n","        cv_results = cross_validate(\n","            estimator = pipeline, \n","            X = df[features], y = df[target], cv = n_folds, \n","            scoring='neg_mean_squared_error',\n","            return_estimator=True\n","            )\n","\n","        neg_mse = cv_results['test_score']  # Negative MSE scores\n","        estimators = cv_results['estimator']  # List of fitted estimators for each fold\n","        \n","        # Loop through each fold result to store the information\n","        for index, (score, estimator) in enumerate(zip(neg_mse, estimators)):\n","            # We access the information associated to the group lasso\n","            group_lasso = estimator.named_steps['group_lasso']\n","            # We store the coefficients estimated by group lasso\n","            coefs = group_lasso.coef_\n","            # We store the intercept estimated by group lasso\n","            intercept = group_lasso.intercept_[0]\n","\n","            # Create a dictionary to store the results for this fold\n","            result_dict = {\n","                'Fold': index + 1,\n","                'Alpha_counter': alpha_counter,\n","                'Alpha': alpha,\n","                'Negative MSE': score,\n","                'Intercept': intercept\n","            }\n","\n","            # Assign coefficients to the corresponding features\n","            for i, feature in enumerate(features):\n","                result_dict[feature] = coefs[i][0]\n","\n","            # Append the result to the list\n","            results_list.append(result_dict)\n","\n","\n","        # Lastly, we remove the last step so that the GroupLasso step does not accumulate\n","        # with different alphas\n","        steps.pop()\n","\n","    # Convert the results list to a DataFrame\n","    results_df = pd.DataFrame(results_list)\n","\n","    return results_df"]},{"cell_type":"markdown","id":"a5743f7e","metadata":{},"source":["Possible improvement: making `group_reg` configurable through parameters (just like looping through alphas)."]},{"cell_type":"markdown","id":"548ea729","metadata":{},"source":["### Training with function 3"]},{"cell_type":"markdown","id":"9ec17d87","metadata":{},"source":["First, we define the group array, establishing which features are part of the same group."]},{"cell_type":"code","execution_count":34,"id":"4f1514ef","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['num_rooms', 'num_baths', 'square_meters', 'year_built', 'num_crimes', 'is_furnished', 'has_pool', 'has_ac', 'accepts_pets', 'orientation_north', 'orientation_south', 'orientation_west', 'neighborhood_Eixample', 'neighborhood_Gràcia', 'neighborhood_Horta', 'neighborhood_Les Cors', 'neighborhood_Nou Barris', 'neighborhood_Sant Andreu', 'neighborhood_Sant Martí', 'neighborhood_Sants', 'neighborhood_Sarrià-Sant Gervasi', 'floor_2', 'floor_3', 'floor_4', 'floor_5', 'floor_6', 'floor_7', 'floor_8', 'floor_9', 'floor_10']\n"]}],"source":["# First, we flatten the list of the features with k-1 dummies\n","features_incomplete_flattened = flatten(features_incomplete)\n","print(features_incomplete_flattened)"]},{"cell_type":"markdown","id":"47da98f2","metadata":{},"source":["Groups (for `GroupLasso`):"]},{"cell_type":"code","execution_count":35,"id":"7465f42a","metadata":{},"outputs":[],"source":["groups_feat_incomp_flat = [\n","    1,  # 'num_rooms'\n","    2,  # 'num_baths'\n","    3,  # 'square_meters'\n","    4,  # 'year_built'\n","    5,  # 'num_crimes'\n","    6,  # 'is_furnished'\n","    7,  # 'has_pool'\n","    8,  # 'has_ac'\n","    9,  # 'accepts_pets'\n","    \n","    # Orientation group\n","    10,  # 'orientation_north'\n","    10,  # 'orientation_south'\n","    10,  # 'orientation_west'\n","    \n","    # Neighborhood group\n","    11,  # 'neighborhood_Eixample'\n","    11,  # 'neighborhood_Gràcia'\n","    11,  # 'neighborhood_Horta'\n","    11,  # 'neighborhood_Les Cors'\n","    11,  # 'neighborhood_Nou Barris'\n","    11,  # 'neighborhood_Sant Andreu'\n","    11,  # 'neighborhood_Sant Martí'\n","    11,  # 'neighborhood_Sants'\n","    11,  # 'neighborhood_Sarrià-Sant Gervasi'\n","\n","    # Floor group\n","    12,  # 'floor_2'\n","    12,  # 'floor_3'\n","    12,  # 'floor_4'\n","    12,  # 'floor_5'\n","    12,  # 'floor_6'\n","    12,  # 'floor_7'\n","    12,  # 'floor_8'\n","    12,  # 'floor_9'\n","    12,  # 'floor_10'\n","]"]},{"cell_type":"code","execution_count":36,"id":"036c9d24","metadata":{},"outputs":[],"source":["# results_v1_f3 = group_lasso(\n","    # df = df_v1, \n","    # features = features_incomplete_flattened,\n","    # target = 'price',\n","    # lasso_groups = groups_feat_incomp_flat,\n","    # n_folds = 5,\n","    # use_knn_imputer = False,\n","    # my_n_neighbors = 5,\n","    # max_n_lasso_iter = 500)\n","\n","# results_v1_f3.to_csv('3_results_datasets/results_v1_f3.csv')"]},{"cell_type":"markdown","id":"0e0066f9","metadata":{},"source":["Approx. 1 minute to execute."]},{"cell_type":"code","execution_count":37,"id":"c4216dcf","metadata":{},"outputs":[],"source":["# results_v1_f3"]},{"cell_type":"markdown","id":"4ad6dd5f","metadata":{},"source":["### Extended version of function 3 with loop for different values of group regularization penalty"]},{"cell_type":"code","execution_count":38,"id":"d8f7dc65","metadata":{},"outputs":[],"source":["def group_lasso_ext(df: pd.DataFrame, features: list, target: str, lasso_groups: list, n_folds: int = 5, max_n_lasso_iter: int = 100):\n","\n","    results_list = []  # To store all results we want for our data frame\n","    alpha_counter = 0  # Initialize a counter for the alphas\n","    group_reg_parameter_counter = 0 # Initialize a counter for the group regularization parameter\n","\n","    # We create the steps of the pipeline where, first, the training data will be standardized\n","    # and, then, the same standardization will be applied to the test data.\n","    # Second, the pipeline will impute null values through KNN, if it is desired.\n","    # Finally, it will perform Group Lasso to the imputed training and test dataset.\n","    steps = [\n","        ('scaler', StandardScaler()),  # Standardize features\n","    ]\n","    \n","    # Now, we create a first loop that iterates over different values of the alpha\n","    # parameter. We generate 50 alphas in log from -10 to 0\n","    for alpha in np.logspace(-10, 0, 50):\n","    \n","        # We add 1 to the counter of alpha each time we consider another alpha\n","        alpha_counter += 1\n","\n","        # We create a second loop that iterates over different values of the \n","        # parameter that penalizes group sparsity\n","        for group_parameter in np.logspace(-10, 0, 50):\n","\n","            # We add Group Lasso to the pipeline, with a specific alpha\n","            steps.append(('group_lasso', \n","                        GroupLasso(\n","                            groups = lasso_groups, \n","                            group_reg = group_parameter, \n","                            l1_reg = alpha, \n","                            fit_intercept = True, \n","                            n_iter = max_n_lasso_iter,\n","                            scale_reg = \"group_size\",\n","                            supress_warning = True\n","                            )))\n","\n","            # We add 1 to the counter of the group regularization parameter in each iteration\n","            group_reg_parameter_counter += 1\n","            \n","            # We create the pipeline with the steps specified above\n","            pipeline = Pipeline(steps)\n","\n","            # Perform cross-validation: on each fold, it will scale and fit the training\n","            # data, and compute the negative mean squared error (so a higher value, i.e.,\n","            # more negative, is better).\n","\n","            cv_results = cross_validate(\n","                estimator = pipeline, \n","                X = df[features], y = df[target], cv = n_folds, \n","                scoring='neg_mean_squared_error',\n","                return_estimator=True\n","                )\n","\n","            neg_mse = cv_results['test_score']  # Negative MSE scores\n","            estimators = cv_results['estimator']  # List of fitted estimators for each fold\n","            \n","            # Loop through each fold result to store the information\n","            for index, (score, estimator) in enumerate(zip(neg_mse, estimators)):\n","                # We access the information associated to the group lasso\n","                group_lasso = estimator.named_steps['group_lasso']\n","                # We store the coefficients estimated by group lasso\n","                coefs = group_lasso.coef_\n","                # We store the intercept estimated by group lasso\n","                intercept = group_lasso.intercept_[0]\n","\n","                # Create a dictionary to store the results for this fold\n","                result_dict = {\n","                    'Fold': index + 1,\n","                    'Alpha_counter': alpha_counter,\n","                    'Group_reg_parameter_counter': group_reg_parameter_counter,\n","                    'Alpha': alpha,\n","                    'Group_reg_parameter': group_parameter,\n","                    'Negative MSE': score,\n","                    'Intercept': intercept\n","                }\n","\n","                # Assign coefficients to the corresponding features\n","                for i, feature in enumerate(features):\n","                    result_dict[feature] = coefs[i][0]\n","\n","                # Append the result to the list\n","                results_list.append(result_dict)\n","    \n","\n","            # Lastly, we remove the last step so that the GroupLasso step does not accumulate\n","            # with different hyperparameters\n","            steps.pop()\n","\n","    # Convert the results list to a DataFrame\n","    results_df = pd.DataFrame(results_list)\n","\n","    return results_df"]},{"cell_type":"markdown","id":"a3632bc9","metadata":{},"source":["## Function 4: features -> flatten list of features -> standardization -> Group LASSO (with alpha parameter tuning + group regularization penalty tuning) -> cross-validation"]},{"cell_type":"markdown","id":"223270d9","metadata":{},"source":["### Addressing the issue of KNN Imputer and categorical variables"]},{"cell_type":"markdown","id":"21e382b4","metadata":{},"source":["1. **KNN Imputer with One-Hot Encoded Dummies**:\n","- The KNN Imputer can be problematic for one-hot encoded categorical variables. The imputer treats all columns as continuous values, so it may impute fractional values for one-hot encoded columns, which is not meaningful (e.g., [0.2, 0.6, 0.2] instead of [0, 1, 0]).\n","- Solution: Before applying the KNN Imputer, you can either:\n","    - Keep categorical variables in their original (labeled) form, impute them, and then re-encode them after the imputation.\n","    - Alternatively, use the `SimpleImputer` with the `most_frequent` strategy for categorical features, and KNN Imputer only for numerical features.\n","2. **Labeled Categorical Variables and Standardization**:\n","- Labeled categorical variables (e.g., blue = 1, red = 2, green = 3) are problematic when standardized because they can imply an ordinal relationship (e.g., green > blue), which may not be valid. Standardization will treat them as continuous values, causing distortions in imputation.\n","- Solution: You should separate categorical and numerical columns and apply different preprocessing strategies to each:\n","    - Numerical Columns: Standardize and then impute with KNN.\n","    - Categorical Columns: Use imputation strategies like `SimpleImputer` or use KNN without standardization.\n","\n","***The most important thing to avoid, in any case, is to impute values including the target variable in the imputation as well!***"]},{"cell_type":"markdown","id":"65c017a1","metadata":{},"source":["### Trying a new approach for combining transformations based on the distinction of numerical and categorical variables"]},{"cell_type":"markdown","id":"05d5bfb9","metadata":{},"source":["#### A. OHE"]},{"cell_type":"code","execution_count":39,"id":"e714e25c","metadata":{},"outputs":[],"source":["# First, we separate categorical and numerical columns. In df_v3, all categorical\n","# variables are stored as objects\n","# categorical_cols = df_v3[features_v3].select_dtypes(include=['category', 'object']).columns.tolist()\n","# numerical_cols = df_v3[features_v3].select_dtypes(include=['float64', 'int64']).columns.tolist()\n","\n","# print(categorical_cols)\n","# print(numerical_cols)"]},{"cell_type":"markdown","id":"a00f872f","metadata":{},"source":["This could be handled by specifying which columns you want to consider as categorical and which as numerical, though, which gives more control in this case given that we don't have many features."]},{"cell_type":"code","execution_count":40,"id":"4441393b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[array(['east', 'north', 'south', 'west', nan], dtype=object), array(['Ciutat Vella', 'Eixample', 'Gràcia', 'Horta', 'Les Cors',\n","       'Nou Barris', 'Sant Andreu', 'Sant Martí', 'Sants',\n","       'Sarrià-Sant Gervasi', nan], dtype=object), array(['1', '10', '2', '3', '4', '5', '6', '7', '8', '9', nan],\n","      dtype=object)]\n"]}],"source":["# How OHE works in scikit-learn\n","ohe = OneHotEncoder(handle_unknown = 'error', sparse_output = False)\n","\n","ohe_array = ohe.fit_transform(df_v3[categorical_cols_v3])\n","\n","print(ohe.categories_)"]},{"cell_type":"code","execution_count":41,"id":"347c62e7","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>orientation_east</th>\n","      <th>orientation_north</th>\n","      <th>orientation_south</th>\n","      <th>orientation_west</th>\n","      <th>orientation_nan</th>\n","      <th>neighborhood_Ciutat Vella</th>\n","      <th>neighborhood_Eixample</th>\n","      <th>neighborhood_Gràcia</th>\n","      <th>neighborhood_Horta</th>\n","      <th>neighborhood_Les Cors</th>\n","      <th>...</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","      <th>floor_nan</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7995</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7996</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7997</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7998</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7999</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8000 rows × 27 columns</p>\n","</div>"],"text/plain":["      orientation_east  orientation_north  orientation_south  \\\n","0                  0.0                0.0                0.0   \n","1                  0.0                0.0                0.0   \n","2                  0.0                0.0                0.0   \n","3                  0.0                0.0                0.0   \n","4                  0.0                0.0                0.0   \n","...                ...                ...                ...   \n","7995               0.0                0.0                1.0   \n","7996               0.0                0.0                0.0   \n","7997               1.0                0.0                0.0   \n","7998               0.0                0.0                0.0   \n","7999               0.0                0.0                0.0   \n","\n","      orientation_west  orientation_nan  neighborhood_Ciutat Vella  \\\n","0                  0.0              1.0                        0.0   \n","1                  1.0              0.0                        0.0   \n","2                  0.0              1.0                        0.0   \n","3                  0.0              1.0                        0.0   \n","4                  0.0              1.0                        0.0   \n","...                ...              ...                        ...   \n","7995               0.0              0.0                        0.0   \n","7996               0.0              1.0                        0.0   \n","7997               0.0              0.0                        0.0   \n","7998               1.0              0.0                        0.0   \n","7999               0.0              1.0                        0.0   \n","\n","      neighborhood_Eixample  neighborhood_Gràcia  neighborhood_Horta  \\\n","0                       0.0                  0.0                 0.0   \n","1                       0.0                  0.0                 0.0   \n","2                       1.0                  0.0                 0.0   \n","3                       0.0                  0.0                 0.0   \n","4                       0.0                  1.0                 0.0   \n","...                     ...                  ...                 ...   \n","7995                    0.0                  0.0                 0.0   \n","7996                    0.0                  0.0                 0.0   \n","7997                    0.0                  0.0                 0.0   \n","7998                    0.0                  1.0                 0.0   \n","7999                    0.0                  0.0                 0.0   \n","\n","      neighborhood_Les Cors  ...  floor_10  floor_2  floor_3  floor_4  \\\n","0                       0.0  ...       0.0      0.0      1.0      0.0   \n","1                       0.0  ...       0.0      0.0      0.0      0.0   \n","2                       0.0  ...       0.0      0.0      0.0      0.0   \n","3                       0.0  ...       0.0      0.0      0.0      0.0   \n","4                       0.0  ...       0.0      0.0      0.0      0.0   \n","...                     ...  ...       ...      ...      ...      ...   \n","7995                    0.0  ...       0.0      0.0      0.0      0.0   \n","7996                    0.0  ...       0.0      0.0      0.0      0.0   \n","7997                    0.0  ...       0.0      0.0      0.0      0.0   \n","7998                    0.0  ...       0.0      0.0      0.0      0.0   \n","7999                    0.0  ...       0.0      0.0      0.0      0.0   \n","\n","      floor_5  floor_6  floor_7  floor_8  floor_9  floor_nan  \n","0         0.0      0.0      0.0      0.0      0.0        0.0  \n","1         0.0      0.0      1.0      0.0      0.0        0.0  \n","2         0.0      0.0      0.0      0.0      0.0        0.0  \n","3         0.0      1.0      0.0      0.0      0.0        0.0  \n","4         1.0      0.0      0.0      0.0      0.0        0.0  \n","...       ...      ...      ...      ...      ...        ...  \n","7995      0.0      0.0      0.0      0.0      1.0        0.0  \n","7996      0.0      0.0      0.0      0.0      1.0        0.0  \n","7997      0.0      0.0      0.0      0.0      1.0        0.0  \n","7998      0.0      0.0      0.0      1.0      0.0        0.0  \n","7999      0.0      0.0      0.0      0.0      0.0        0.0  \n","\n","[8000 rows x 27 columns]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# Get the feature names from OneHotEncoder\n","ohe_feature_names = ohe.get_feature_names_out(categorical_cols_v3)\n","\n","# Convert the array back to a DataFrame\n","ohe_df = pd.DataFrame(ohe_array, columns=ohe_feature_names, index=df_v3.index)\n","ohe_df\n","\n","# Now you could concatenate this back with the original dataframe (excluding the original categorical columns)\n","# df_v3_encoded = pd.concat([df_v3.drop(categorical_cols, axis=1), ohe_df], axis=1)"]},{"cell_type":"markdown","id":"a3100bfe","metadata":{},"source":["OHE of scikit-learn cannot handle null values, as it will consider those as a category by themselves. That is also a reason why imputing must be done *before* encoding. Note, however, that dummy variables are treated again as variables to encode: we must handle that, maybe by excluding them from OHE. For more information, see https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.OneHotEncoder.html. Probably, the best thing to do is to indicate, manually, which columns are to be considered categorical, which numerical and which boolean."]},{"cell_type":"markdown","id":"a031f958","metadata":{},"source":["#### B. Different transformations and imputations with `ColumnTransformer`"]},{"cell_type":"markdown","id":"c8ec1070","metadata":{},"source":["Below, we are implementing the separate transformation into a pipeline through `Pipeline` and `ColumnTransformer`.\n","- Documentation for `ColumnTransformer`: https://scikit-learn.org/1.5/modules/generated/sklearn.compose.ColumnTransformer.html. According to the documentation, \"This estimator allows different columns or column subsets of the input to be transformed ***separately*** and **the features generated by each transformer will be concatenated to form a single feature space**. This is **useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer**.\"\n","- Documentation for `Pipeline`: https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html. More information and intuition on the implementation in previous functions.\n","\n","Specifically, we are doing the following:\n","1. We are creating two lists with tuples inside where we store the specific transformation for each type of variable (standardization for numerical and OHE for categorical).\n","2. Then, we are inserting (through `insert`: https://docs.python.org/3/tutorial/datastructures.html), in different places of the list that will act as the steps of the preprocessor, the imputing method (KNN for numerical, mode or most frequent value for categorical variables and dummies). Note how we are inserting the tuple in the same way as the `Pipeline` expects. `SimpleImputer` doc https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\n","3. We create a ColumnTransformer class for being able to apply different transforming and/or imputation strategies to different features.\n","4. Finally, we create a list where we will store the steps for the pipeline where we will insert Group Lasso."]},{"cell_type":"code","execution_count":42,"id":"04c5c748","metadata":{},"outputs":[],"source":["# Create transformers for categorical and numerical features\n","numerical_transformer = [\n","    ('scaler', StandardScaler())  # Standardize numerical features\n","    ]\n","    \n","categorical_transformer = [\n","    ('onehot', OneHotEncoder(handle_unknown='error', drop = 'first'))  # One-hot encode categorical features, dropping first dummy\n","]\n","\n","my_n_neighbors = 5\n","\n","# Add KNN imputer for numerical features. We append it, so that standardization is\n","# performed before KNN Imputing\n","numerical_transformer.append(('knn_imputer', KNNImputer(n_neighbors = my_n_neighbors)))\n","# Optionally, use SimpleImputer for categorical features instead of KNN. We insert it \n","# BEFORE one-hot encoding the categorical variables\n","categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","\n","# Create the ColumnTransformer to apply different transformations to numerical\n","# columns, categorical and dummy (boolean) columns\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', Pipeline(numerical_transformer), numerical_cols_v3),\n","        ('cat', Pipeline(categorical_transformer), categorical_cols_v3),\n","        ('bool', SimpleImputer(strategy='most_frequent'), boolean_cols_v3)\n","    ]\n",")\n","\n","# Create steps of the pipeline\n","steps = [('preprocessor', preprocessor)]"]},{"cell_type":"markdown","id":"5d9ca3e7","metadata":{},"source":["##### Testing with fake data"]},{"cell_type":"markdown","id":"0354a3d3","metadata":{},"source":["Now, we can test whether this pipeline with a `ColumnTransformer` works, first with some sample data and then with our data: "]},{"cell_type":"code","execution_count":43,"id":"b19745d4","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>numerical_1</th>\n","      <th>numerical_2</th>\n","      <th>categorical_1_B</th>\n","      <th>categorical_2_Y</th>\n","      <th>categorical_2_Z</th>\n","      <th>boolean_1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.069045e+00</td>\n","      <td>-1.336306e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-2.672612e-01</td>\n","      <td>-2.590520e-16</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.480297e-16</td>\n","      <td>2.672612e-01</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.336306e+00</td>\n","      <td>1.069045e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    numerical_1   numerical_2  categorical_1_B  categorical_2_Y  \\\n","0 -1.069045e+00 -1.336306e+00              0.0              0.0   \n","1 -2.672612e-01 -2.590520e-16              1.0              0.0   \n","2 -1.480297e-16  2.672612e-01              0.0              1.0   \n","3  1.336306e+00  1.069045e+00              0.0              0.0   \n","\n","   categorical_2_Z  boolean_1  \n","0              0.0        1.0  \n","1              0.0        0.0  \n","2              0.0        1.0  \n","3              1.0        1.0  "]},"metadata":{},"output_type":"display_data"}],"source":["# Sample DataFrame\n","data = {\n","    'numerical_1': [1, 2, np.nan, 4],\n","    'numerical_2': [5, np.nan, 7, 8],\n","    'categorical_1': ['A', 'B', 'A', np.nan],\n","    'categorical_2': ['X', np.nan, 'Y', 'Z'],\n","    'boolean_1': [1, 0, np.nan, 1]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Define the column lists\n","numerical_cols_test = ['numerical_1', 'numerical_2']\n","categorical_cols_test = ['categorical_1', 'categorical_2']\n","boolean_cols_test = ['boolean_1']\n","\n","###############################################################################\n","\n","# Create transformers for categorical and numerical features\n","numerical_transformer = [\n","    ('scaler', StandardScaler())  # Standardize numerical features\n","    ]\n","    \n","categorical_transformer = [\n","    ('onehot', OneHotEncoder(handle_unknown='error', drop = 'first'))  # One-hot encode categorical features, dropping first dummy\n","]\n","\n","my_n_neighbors = 5\n","\n","# Add KNN imputer for numerical features. We append it, so that standardization is\n","# performed before KNN Imputing\n","numerical_transformer.append(('knn_imputer', KNNImputer(n_neighbors = my_n_neighbors)))\n","# Optionally, use SimpleImputer for categorical features instead of KNN. We insert it \n","# BEFORE one-hot encoding the categorical variables\n","categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","\n","# Create the ColumnTransformer to apply different transformations to numerical\n","# columns, categorical and dummy (boolean) columns\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', Pipeline(numerical_transformer), numerical_cols_test),\n","        ('cat', Pipeline(categorical_transformer), categorical_cols_test),\n","        ('bool', SimpleImputer(strategy='most_frequent'), boolean_cols_test)\n","    ]\n",")\n","\n","# Create steps of the pipeline\n","steps = [('preprocessor', preprocessor)]\n","\n","###############################################################################\n","\n","# Fit the pipeline to the sample data\n","pipeline_test = Pipeline(steps)\n","pipeline_test.fit(df)\n","\n","# Apply the transformations\n","transformed_data_test = pipeline_test.transform(df)\n","\n","# Get feature names after transformations\n","numerical_feature_names_test = numerical_cols_test  # They remain the same after scaling\n","categorical_feature_names_test = pipeline_test.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols_test)\n","boolean_feature_names_test = boolean_cols_test  # They remain the same after imputation\n","\n","# Combine all feature names\n","all_feature_names_test = list(numerical_feature_names_test) + list(categorical_feature_names_test) + list(boolean_feature_names_test)\n","\n","# Convert the transformed data to a DataFrame with proper column names\n","transformed_df_test = pd.DataFrame(transformed_data_test, columns=all_feature_names_test)\n","display(transformed_df_test)"]},{"cell_type":"markdown","id":"aab8dd07","metadata":{},"source":["This first attempt seems to have worked. All the transformations have been done as expected. Now, let's see how it performs with our data."]},{"cell_type":"markdown","id":"1816b3a5","metadata":{},"source":["##### Testing with our data"]},{"cell_type":"code","execution_count":44,"id":"bf43b111","metadata":{},"outputs":[{"ename":"ValueError","evalue":"A given column is not a column of the dataframe","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'numerical_1'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/_indexing.py:361\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 361\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m all_columns\u001b[38;5;241m.\u001b[39mget_loc(col)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n","\u001b[0;31mKeyError\u001b[0m: 'numerical_1'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the pipeline to the sample data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps)\n\u001b[0;32m----> 3\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(df_v3[features_v3])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Apply the transformations\u001b[39;00m\n\u001b[1;32m      6\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtransform(df_v3[features_v3])\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:922\u001b[0m, in \u001b[0;36mColumnTransformer.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    919\u001b[0m _raise_for_params(params, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# need the transformed data) to have consistent output type in predict\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_transform(X, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:968\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m    966\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m--> 968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:536\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    534\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    535\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 536\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m _get_column_indices(X, columns)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n","File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/_indexing.py:369\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    366\u001b[0m         column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n","\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"]}],"source":["# Fit the pipeline to the sample data\n","pipeline = Pipeline(steps)\n","pipeline.fit(df_v3[features_v3])\n","\n","# Apply the transformations\n","transformed_data = pipeline.transform(df_v3[features_v3])\n","\n","# Get feature names after transformations\n","numerical_feature_names = numerical_cols_v3  # They remain the same after scaling\n","categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols_v3)\n","boolean_feature_names = boolean_cols_v3  # They remain the same after imputation\n","\n","# Combine all feature names\n","all_feature_names = list(numerical_feature_names) + list(boolean_feature_names) + list(categorical_feature_names) \n","\n","# Convert the transformed data to a DataFrame with proper column names\n","transformed_df = pd.DataFrame(transformed_data, columns=all_feature_names)\n","\n","# We enforce boolean and categorical columns to be interpreted as integers, and numerical as floats\n","transformed_df[numerical_feature_names] = transformed_df[numerical_feature_names].astype(float)\n","transformed_df[boolean_feature_names] = transformed_df[boolean_feature_names].astype(int)\n","transformed_df[categorical_feature_names] = transformed_df[categorical_feature_names].astype(int)\n","\n","display(transformed_df)"]},{"cell_type":"code","execution_count":71,"id":"36c90311","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['num_rooms', 'num_baths', 'square_meters', 'year_built', 'num_crimes',\n","       'is_furnished', 'has_pool', 'has_ac', 'accepts_pets',\n","       'orientation_north', 'orientation_south', 'orientation_west',\n","       'neighborhood_Eixample', 'neighborhood_Gràcia', 'neighborhood_Horta',\n","       'neighborhood_Les Cors', 'neighborhood_Nou Barris',\n","       'neighborhood_Sant Andreu', 'neighborhood_Sant Martí',\n","       'neighborhood_Sants', 'neighborhood_Sarrià-Sant Gervasi', 'floor_10',\n","       'floor_2', 'floor_3', 'floor_4', 'floor_5', 'floor_6', 'floor_7',\n","       'floor_8', 'floor_9'],\n","      dtype='object')\n"]}],"source":["print(transformed_df.columns)"]},{"cell_type":"code","execution_count":97,"id":"1357d3d0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 8000 entries, 0 to 7999\n","Data columns (total 30 columns):\n"," #   Column                            Non-Null Count  Dtype  \n","---  ------                            --------------  -----  \n"," 0   num_rooms                         8000 non-null   float64\n"," 1   num_baths                         8000 non-null   float64\n"," 2   square_meters                     8000 non-null   float64\n"," 3   year_built                        8000 non-null   float64\n"," 4   num_crimes                        8000 non-null   float64\n"," 5   is_furnished                      8000 non-null   int64  \n"," 6   has_pool                          8000 non-null   int64  \n"," 7   has_ac                            8000 non-null   int64  \n"," 8   accepts_pets                      8000 non-null   int64  \n"," 9   orientation_north                 8000 non-null   int64  \n"," 10  orientation_south                 8000 non-null   int64  \n"," 11  orientation_west                  8000 non-null   int64  \n"," 12  neighborhood_Eixample             8000 non-null   int64  \n"," 13  neighborhood_Gràcia               8000 non-null   int64  \n"," 14  neighborhood_Horta                8000 non-null   int64  \n"," 15  neighborhood_Les Cors             8000 non-null   int64  \n"," 16  neighborhood_Nou Barris           8000 non-null   int64  \n"," 17  neighborhood_Sant Andreu          8000 non-null   int64  \n"," 18  neighborhood_Sant Martí           8000 non-null   int64  \n"," 19  neighborhood_Sants                8000 non-null   int64  \n"," 20  neighborhood_Sarrià-Sant Gervasi  8000 non-null   int64  \n"," 21  floor_10                          8000 non-null   int64  \n"," 22  floor_2                           8000 non-null   int64  \n"," 23  floor_3                           8000 non-null   int64  \n"," 24  floor_4                           8000 non-null   int64  \n"," 25  floor_5                           8000 non-null   int64  \n"," 26  floor_6                           8000 non-null   int64  \n"," 27  floor_7                           8000 non-null   int64  \n"," 28  floor_8                           8000 non-null   int64  \n"," 29  floor_9                           8000 non-null   int64  \n","dtypes: float64(5), int64(25)\n","memory usage: 1.8 MB\n","None\n"]}],"source":["print(transformed_df.info())"]},{"cell_type":"markdown","id":"b437c814","metadata":{},"source":["Everything seems to work as expected!"]},{"cell_type":"markdown","id":"9db43b54","metadata":{},"source":["#### C. Checking whether `cross_validate` applies the transformations and imputations separately to the training and test dataset\n","The idea now is to extend the example above and check whether the pipeline works also when applying cross-validation."]},{"cell_type":"code","execution_count":82,"id":"7a7cc47d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n","    scores = scorer(estimator, X_test, y_test, **score_params)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n","    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n","    response_method = _check_response_method(estimator, self._response_method)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n","    raise AttributeError(\n","AttributeError: Pipeline has none of the following attributes: predict.\n","\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n","    scores = scorer(estimator, X_test, y_test, **score_params)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n","    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n","    response_method = _check_response_method(estimator, self._response_method)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n","    raise AttributeError(\n","AttributeError: Pipeline has none of the following attributes: predict.\n","\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n","    scores = scorer(estimator, X_test, y_test, **score_params)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n","    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n","    response_method = _check_response_method(estimator, self._response_method)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n","    raise AttributeError(\n","AttributeError: Pipeline has none of the following attributes: predict.\n","\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n","    scores = scorer(estimator, X_test, y_test, **score_params)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n","    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n","    response_method = _check_response_method(estimator, self._response_method)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n","    raise AttributeError(\n","AttributeError: Pipeline has none of the following attributes: predict.\n","\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n","    scores = scorer(estimator, X_test, y_test, **score_params)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n","    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n","    response_method = _check_response_method(estimator, self._response_method)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n","    raise AttributeError(\n","AttributeError: Pipeline has none of the following attributes: predict.\n","\n","  warnings.warn(\n"]}],"source":["# Create transformers for categorical and numerical features\n","numerical_transformer = [\n","    ('scaler', StandardScaler())  # Standardize numerical features\n","    ]\n","    \n","categorical_transformer = [\n","    ('onehot', OneHotEncoder(handle_unknown='error', drop = 'first'))  # One-hot encode categorical features, dropping first dummy\n","]\n","\n","my_n_neighbors = 5\n","\n","# Add KNN imputer for numerical features. We append it, so that standardization is\n","# performed before KNN Imputing\n","numerical_transformer.append(('knn_imputer', KNNImputer(n_neighbors = my_n_neighbors)))\n","# Optionally, use SimpleImputer for categorical features instead of KNN. We insert it \n","# BEFORE one-hot encoding the categorical variables\n","categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","\n","# Create the ColumnTransformer to apply different transformations to numerical\n","# columns, categorical and dummy (boolean) columns\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', Pipeline(numerical_transformer), numerical_cols_v3),\n","        ('cat', Pipeline(categorical_transformer), categorical_cols_v3),\n","        ('bool', SimpleImputer(strategy='most_frequent'), boolean_cols_v3)\n","    ]\n",")\n","\n","# Create steps of the pipeline\n","steps = [('preprocessor', preprocessor)]\n","\n","###############################################################################\n","\n","# Fit the pipeline to our data\n","pipeline = Pipeline(steps)\n","\n","###############################################################################\n","\n","# We perform cross-validation\n","\n","target = 'price'\n","n_folds = 5\n","\n","cv_results = cross_validate(\n","                estimator = pipeline, \n","                X = df_v3[features_v3], y = df_v3[target], cv = n_folds, \n","                scoring='neg_mean_squared_error',\n","                return_estimator=True\n","                )"]},{"cell_type":"markdown","id":"23fa8f73","metadata":{},"source":["The error above appears because there is no class in the pipeline with the predict attribute. This should be resolved when adding GroupLasso to the equation."]},{"cell_type":"markdown","id":"eb0732ed","metadata":{},"source":["To ensure that your pipeline handles transformations correctly in the cross-validation process, you can take these steps to verify if each step behaves as expected in avoiding data leakage:\n","\n","1. **Verify Transformation Steps on Estimators in Each Fold**: Since `cross_validate` with `return_estimator=True` will return fitted estimators for each fold, you can inspect each estimator after cross-validation to confirm the behavior of your pipeline components."]},{"cell_type":"code","execution_count":83,"id":"0506422b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Estimator 1\n","Scaler mean: [   2.73296875    1.99585195  109.06858054 1985.33067348    2.81066411]\n","Scaler variance: [1.81957256e+01 6.65585857e-01 1.23214362e+03 4.21786601e+02\n"," 1.19069999e+01]\n","KNN Imputer statistics: KNNImputer()\n","SimpleImputer most frequent: ['west' 'Sants' '5']\n","Estimator 2\n","Scaler mean: [2.85343750e+00 1.98232765e+00 1.09466486e+02 1.98506523e+03\n"," 2.79326846e+00]\n","Scaler variance: [2.77491444e+01 6.64709182e-01 1.27055657e+03 4.30058095e+02\n"," 1.17955138e+01]\n","KNN Imputer statistics: KNNImputer()\n","SimpleImputer most frequent: ['west' 'Sants' '6']\n","Estimator 3\n","Scaler mean: [   2.894375      1.9856528   109.18818659 1985.37001595    2.8181963 ]\n","Scaler variance: [2.92738434e+01 6.66460824e-01 1.25215436e+03 4.22537729e+02\n"," 1.19588212e+01]\n","KNN Imputer statistics: KNNImputer()\n","SimpleImputer most frequent: ['west' 'Nou Barris' '5']\n","Estimator 4\n","Scaler mean: [   2.92578125    1.99058861  109.06782359 1985.39105431    2.80592734]\n","Scaler variance: [3.21890228e+01 6.64291710e-01 1.26048322e+03 4.23971677e+02\n"," 1.19046558e+01]\n","KNN Imputer statistics: KNNImputer()\n","SimpleImputer most frequent: ['west' 'Nou Barris' '5']\n","Estimator 5\n","Scaler mean: [   2.9359375     1.98691976  108.99984082 1985.44791833    2.78916335]\n","Scaler variance: [3.30224585e+01 6.67239978e-01 1.23812910e+03 4.21335499e+02\n"," 1.18594523e+01]\n","KNN Imputer statistics: KNNImputer()\n","SimpleImputer most frequent: ['west' 'Sants' '6']\n"]}],"source":["for idx, estimator in enumerate(cv_results['estimator']):\n","    print(f\"Estimator {idx+1}\")\n","    # Check numerical scaler mean and variance for each fold\n","    print(\"Scaler mean:\", estimator.named_steps['preprocessor'].named_transformers_['num'].named_steps['scaler'].mean_)\n","    print(\"Scaler variance:\", estimator.named_steps['preprocessor'].named_transformers_['num'].named_steps['scaler'].var_)\n","    # Check KNN Imputer for fitted values\n","    print(\"KNN Imputer statistics:\", estimator.named_steps['preprocessor'].named_transformers_['num'].named_steps['knn_imputer'])\n","    # Check categorical most frequent values\n","    print(\"SimpleImputer most frequent:\", estimator.named_steps['preprocessor'].named_transformers_['cat'].named_steps['simple_imputer'].statistics_)"]},{"cell_type":"markdown","id":"8f8775c9","metadata":{},"source":["Your output indicates that the pipeline components are working independently within each cross-validation fold, which helps avoid data leakage. Let’s review each part of the output in light of your expectations:\n","\n","1. **Scaler Mean and Variance**:\n","   - Each estimator shows different means and variances for the `StandardScaler`, which suggests that the scaling is being calculated separately for each training fold. This is expected and desired, as it means the scaler is not influenced by data in the validation set.\n","   \n","2. **KNN Imputer**:\n","   - Since `KNNImputer()` doesn’t return specific statistics, it simply shows as `KNNImputer()`. However, its presence here indicates it is applied independently in each fold, as expected. The variation in `StandardScaler` parameters across folds implies that KNN Imputation is applied after scaling, which aligns with your specified order.\n","\n","3. **SimpleImputer (Most Frequent) for Categorical Variables**:\n","   - The output shows different “most frequent” values in each fold (e.g., \"Sants\" vs. \"Nou Barris\" and \"5\" vs. \"6\"), which confirms that `SimpleImputer` is computing the mode independently for each training fold. This avoids any leakage of the most frequent categories from the validation set.\n","\n","4. **SimpleImputer for Boolean Columns**:\n","   - If boolean columns had specific values imputed, you would expect to see similar changes here as you see for categorical columns, with the most frequent values varying slightly across folds (if they were not specified in your code block).\n","\n","**Summary**\n","\n","Everything appears correct:\n","- **Scaling** is fold-specific, ensuring no leakage.\n","- **KNN Imputation** respects fold boundaries.\n","- **Categorical and Boolean Imputation** is fold-specific, confirming independent fitting.\n","\n","Your cross-validation setup is performing as expected.\n","\n","---"]},{"cell_type":"markdown","id":"a47bc18d","metadata":{},"source":["2. **Ensure Consistency of One-Hot Encoding**: Verify that OneHotEncoder has consistent feature names across folds. Inconsistent feature names could indicate that categories were seen in some folds but not others, potentially leading to data leakage. You can check the categories for each fold:"]},{"cell_type":"code","execution_count":84,"id":"78509f4f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Estimator 1\n","Categories for OneHotEncoder: [array(['east', 'north', 'south', 'west'], dtype=object), array(['Ciutat Vella', 'Eixample', 'Gràcia', 'Horta', 'Les Cors',\n","       'Nou Barris', 'Sant Andreu', 'Sant Martí', 'Sants',\n","       'Sarrià-Sant Gervasi'], dtype=object), array(['1', '10', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)]\n","Estimator 2\n","Categories for OneHotEncoder: [array(['east', 'north', 'south', 'west'], dtype=object), array(['Ciutat Vella', 'Eixample', 'Gràcia', 'Horta', 'Les Cors',\n","       'Nou Barris', 'Sant Andreu', 'Sant Martí', 'Sants',\n","       'Sarrià-Sant Gervasi'], dtype=object), array(['1', '10', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)]\n","Estimator 3\n","Categories for OneHotEncoder: [array(['east', 'north', 'south', 'west'], dtype=object), array(['Ciutat Vella', 'Eixample', 'Gràcia', 'Horta', 'Les Cors',\n","       'Nou Barris', 'Sant Andreu', 'Sant Martí', 'Sants',\n","       'Sarrià-Sant Gervasi'], dtype=object), array(['1', '10', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)]\n","Estimator 4\n","Categories for OneHotEncoder: [array(['east', 'north', 'south', 'west'], dtype=object), array(['Ciutat Vella', 'Eixample', 'Gràcia', 'Horta', 'Les Cors',\n","       'Nou Barris', 'Sant Andreu', 'Sant Martí', 'Sants',\n","       'Sarrià-Sant Gervasi'], dtype=object), array(['1', '10', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)]\n","Estimator 5\n","Categories for OneHotEncoder: [array(['east', 'north', 'south', 'west'], dtype=object), array(['Ciutat Vella', 'Eixample', 'Gràcia', 'Horta', 'Les Cors',\n","       'Nou Barris', 'Sant Andreu', 'Sant Martí', 'Sants',\n","       'Sarrià-Sant Gervasi'], dtype=object), array(['1', '10', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)]\n"]}],"source":["for idx, estimator in enumerate(cv_results['estimator']):\n","    print(f\"Estimator {idx+1}\")\n","    ohe = estimator.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n","    print(\"Categories for OneHotEncoder:\", ohe.categories_)"]},{"cell_type":"markdown","id":"3f3c0a71","metadata":{},"source":["It looks like the categories output from `OneHotEncoder` includes the full list rather than showing that it has dropped the first category, despite `drop='first'` being set. This happens because `OneHotEncoder` will still display the full list of categories in `categories_` even if the first category is dropped during encoding. The `categories_` attribute lists all categories found in each feature, not the columns that will actually appear in the transformed data.\n","\n","To verify that `OneHotEncoder` is correctly dropping the first dummy in each category, you can inspect the transformed output shape or examine the feature names after transformation. Here’s how you can check:\n","\n","After transforming a sample dataset, retrieve the feature names to confirm which dummies are actually included:\n","\n","This should confirm that the first category in each feature is indeed being dropped in the actual encoded output."]},{"cell_type":"code","execution_count":85,"id":"17f6b0a8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Estimator 1 OneHotEncoder feature names: ['orientation_north' 'orientation_south' 'orientation_west'\n"," 'neighborhood_Eixample' 'neighborhood_Gràcia' 'neighborhood_Horta'\n"," 'neighborhood_Les Cors' 'neighborhood_Nou Barris'\n"," 'neighborhood_Sant Andreu' 'neighborhood_Sant Martí' 'neighborhood_Sants'\n"," 'neighborhood_Sarrià-Sant Gervasi' 'floor_10' 'floor_2' 'floor_3'\n"," 'floor_4' 'floor_5' 'floor_6' 'floor_7' 'floor_8' 'floor_9']\n","Estimator 2 OneHotEncoder feature names: ['orientation_north' 'orientation_south' 'orientation_west'\n"," 'neighborhood_Eixample' 'neighborhood_Gràcia' 'neighborhood_Horta'\n"," 'neighborhood_Les Cors' 'neighborhood_Nou Barris'\n"," 'neighborhood_Sant Andreu' 'neighborhood_Sant Martí' 'neighborhood_Sants'\n"," 'neighborhood_Sarrià-Sant Gervasi' 'floor_10' 'floor_2' 'floor_3'\n"," 'floor_4' 'floor_5' 'floor_6' 'floor_7' 'floor_8' 'floor_9']\n","Estimator 3 OneHotEncoder feature names: ['orientation_north' 'orientation_south' 'orientation_west'\n"," 'neighborhood_Eixample' 'neighborhood_Gràcia' 'neighborhood_Horta'\n"," 'neighborhood_Les Cors' 'neighborhood_Nou Barris'\n"," 'neighborhood_Sant Andreu' 'neighborhood_Sant Martí' 'neighborhood_Sants'\n"," 'neighborhood_Sarrià-Sant Gervasi' 'floor_10' 'floor_2' 'floor_3'\n"," 'floor_4' 'floor_5' 'floor_6' 'floor_7' 'floor_8' 'floor_9']\n","Estimator 4 OneHotEncoder feature names: ['orientation_north' 'orientation_south' 'orientation_west'\n"," 'neighborhood_Eixample' 'neighborhood_Gràcia' 'neighborhood_Horta'\n"," 'neighborhood_Les Cors' 'neighborhood_Nou Barris'\n"," 'neighborhood_Sant Andreu' 'neighborhood_Sant Martí' 'neighborhood_Sants'\n"," 'neighborhood_Sarrià-Sant Gervasi' 'floor_10' 'floor_2' 'floor_3'\n"," 'floor_4' 'floor_5' 'floor_6' 'floor_7' 'floor_8' 'floor_9']\n","Estimator 5 OneHotEncoder feature names: ['orientation_north' 'orientation_south' 'orientation_west'\n"," 'neighborhood_Eixample' 'neighborhood_Gràcia' 'neighborhood_Horta'\n"," 'neighborhood_Les Cors' 'neighborhood_Nou Barris'\n"," 'neighborhood_Sant Andreu' 'neighborhood_Sant Martí' 'neighborhood_Sants'\n"," 'neighborhood_Sarrià-Sant Gervasi' 'floor_10' 'floor_2' 'floor_3'\n"," 'floor_4' 'floor_5' 'floor_6' 'floor_7' 'floor_8' 'floor_9']\n"]}],"source":["for idx, estimator in enumerate(cv_results['estimator']):\n","    ohe_feature_names = estimator.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols_v3)\n","    print(f\"Estimator {idx+1} OneHotEncoder feature names: {ohe_feature_names}\")"]},{"cell_type":"markdown","id":"926fe2a6","metadata":{},"source":["Therefore, the OHE is working as expected!\n","\n","---"]},{"cell_type":"markdown","id":"695e1187","metadata":{},"source":["3. **Confirm Imputation Consistency for Boolean Columns**: Ensure that `SimpleImputer` for boolean columns applies correctly in each fold by comparing the statistics it computed:"]},{"cell_type":"code","execution_count":86,"id":"871ed0f4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Estimator 1\n","Most frequent values for Boolean Imputer: [False True True True]\n","Estimator 2\n","Most frequent values for Boolean Imputer: [False True False True]\n","Estimator 3\n","Most frequent values for Boolean Imputer: [False True True True]\n","Estimator 4\n","Most frequent values for Boolean Imputer: [False True True True]\n","Estimator 5\n","Most frequent values for Boolean Imputer: [False True True True]\n"]}],"source":["for idx, estimator in enumerate(cv_results['estimator']):\n","    print(f\"Estimator {idx+1}\")\n","    boolean_imputer = estimator.named_steps['preprocessor'].named_transformers_['bool']\n","    print(\"Most frequent values for Boolean Imputer:\", boolean_imputer.statistics_)"]},{"cell_type":"markdown","id":"97eb81cc","metadata":{},"source":["Yes, this output suggests that the `SimpleImputer` for boolean columns is working as expected. Each fold has a slightly different set of \"most frequent\" values across the boolean columns, which means:\n","\n","1. **Fold-Specific Computation**: The most frequent values vary slightly across folds, which implies that SimpleImputer is independently calculating the mode for each fold's training set rather than using the entire dataset, avoiding data leakage.\n","\n","2. **Consistent Imputation**: Although you see some consistent values (`True` or `False` in most cases), the occasional change (like `False` in Estimator 2 vs. `True` in others) indicates that imputation is adjusted according to each training subset.\n","\n","Overall, this output confirms that boolean columns are being imputed correctly, using values calculated independently per fold, ensuring no leakage from the validation set."]},{"cell_type":"markdown","id":"85a410ff","metadata":{},"source":["#### D. Designing a dynamic way to define the number of groups created by categorical variables for `GroupLasso`"]},{"cell_type":"code","execution_count":28,"id":"a058e724","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>num_crimes</th>\n","      <th>is_furnished</th>\n","      <th>has_pool</th>\n","      <th>has_ac</th>\n","      <th>accepts_pets</th>\n","      <th>orientation</th>\n","      <th>neighborhood</th>\n","      <th>floor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>1956.0</td>\n","      <td>2.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>NaN</td>\n","      <td>Sant Martí</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>133.0</td>\n","      <td>1960.0</td>\n","      <td>4.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>west</td>\n","      <td>Sants</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>137.0</td>\n","      <td>2000.0</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>Eixample</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>41.0</td>\n","      <td>2002.0</td>\n","      <td>5.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>Sants</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>70.0</td>\n","      <td>1979.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>NaN</td>\n","      <td>Gràcia</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7995</th>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>95.0</td>\n","      <td>2021.0</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>south</td>\n","      <td>Sant Martí</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7996</th>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>166.0</td>\n","      <td>1995.0</td>\n","      <td>9.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>Nou Barris</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7997</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>89.0</td>\n","      <td>2015.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>east</td>\n","      <td>Sant Martí</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7998</th>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>167.0</td>\n","      <td>1977.0</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>west</td>\n","      <td>Gràcia</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>7999</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>95.0</td>\n","      <td>2000.0</td>\n","      <td>8.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>Nou Barris</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8000 rows × 12 columns</p>\n","</div>"],"text/plain":["      num_rooms  num_baths  square_meters  year_built  num_crimes  \\\n","0           1.0        1.0            NaN      1956.0         2.0   \n","1           4.0        1.0          133.0      1960.0         4.0   \n","2           2.0        3.0          137.0      2000.0         0.0   \n","3           1.0        2.0           41.0      2002.0         5.0   \n","4           2.0        1.0           70.0      1979.0         0.0   \n","...         ...        ...            ...         ...         ...   \n","7995        2.0        NaN           95.0      2021.0         0.0   \n","7996        4.0        3.0          166.0      1995.0         9.0   \n","7997        4.0        1.0           89.0      2015.0         0.0   \n","7998        4.0        2.0          167.0      1977.0         0.0   \n","7999        2.0        1.0           95.0      2000.0         8.0   \n","\n","     is_furnished has_pool has_ac accepts_pets orientation neighborhood floor  \n","0           False    False   True         True         NaN   Sant Martí     3  \n","1           False    False  False        False        west        Sants     7  \n","2           False     True  False        False         NaN     Eixample     1  \n","3           False     True  False        False         NaN        Sants     6  \n","4            True    False  False         True         NaN       Gràcia     5  \n","...           ...      ...    ...          ...         ...          ...   ...  \n","7995        False    False  False        False       south   Sant Martí     9  \n","7996        False    False  False        False         NaN   Nou Barris     9  \n","7997         True    False   True        False        east   Sant Martí     9  \n","7998        False     True   True        False        west       Gràcia     8  \n","7999        False     True   True        False         NaN   Nou Barris     1  \n","\n","[8000 rows x 12 columns]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# Checking that, when passing in a certain order the names of the features to a\n","# data frame, it will return the data frame with the features in that order\n","df_v3[numerical_cols_v3 + boolean_cols_v3 + categorical_cols_v3]"]},{"cell_type":"markdown","id":"d0f54786","metadata":{},"source":["Since the idea is to define a list for `GroupLasso` which specifies which variables are of the same group, which looks like:"]},{"cell_type":"code","execution_count":73,"id":"ff8063d6","metadata":{},"outputs":[],"source":["groups_feat_incomp_flat = [\n","    1,  # 'num_rooms'\n","    2,  # 'num_baths'\n","    3,  # 'square_meters'\n","    4,  # 'year_built'\n","    5,  # 'num_crimes'\n","    6,  # 'is_furnished'\n","    7,  # 'has_pool'\n","    8,  # 'has_ac'\n","    9,  # 'accepts_pets'\n","    \n","    # Orientation group\n","    10,  # 'orientation_north'\n","    10,  # 'orientation_south'\n","    10,  # 'orientation_west'\n","    \n","    # Neighborhood group\n","    11,  # 'neighborhood_Eixample'\n","    11,  # 'neighborhood_Gràcia'\n","    11,  # 'neighborhood_Horta'\n","    11,  # 'neighborhood_Les Cors'\n","    11,  # 'neighborhood_Nou Barris'\n","    11,  # 'neighborhood_Sant Andreu'\n","    11,  # 'neighborhood_Sant Martí'\n","    11,  # 'neighborhood_Sants'\n","    11,  # 'neighborhood_Sarrià-Sant Gervasi'\n","\n","    # Floor group\n","    12,  # 'floor_2'\n","    12,  # 'floor_3'\n","    12,  # 'floor_4'\n","    12,  # 'floor_5'\n","    12,  # 'floor_6'\n","    12,  # 'floor_7'\n","    12,  # 'floor_8'\n","    12,  # 'floor_9'\n","    12,  # 'floor_10'\n","]"]},{"cell_type":"markdown","id":"0b40ada9","metadata":{},"source":["Then, by passing to `cross-validate` (which is where `GroupLasso` is applied, *after* preprocessing) a dataframe with the order: 1) Numerical columns, 2) Boolean (dummy) columns and 3) Categorical columns, the groups can be defined as 1 number, starting from 1, to numerical columns and boolean columns, and then 1 number per categorical variable which repeats n-1 times, where n = number of unique categories, excluding null values. "]},{"cell_type":"code","execution_count":33,"id":"b4bcbc85","metadata":{},"outputs":[{"data":{"text/plain":["[1, 2, 3, 4, 5]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["groups_numerical_vars = [x for x in range(1, len(numerical_cols_v3) + 1)] # We add 1 to the end of the range as range() stops before the last number\n","groups_numerical_vars"]},{"cell_type":"code","execution_count":35,"id":"e40175a8","metadata":{},"outputs":[{"data":{"text/plain":["[1, 2, 3, 4]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["groups_bool_vars = [x for x in range(1, len(boolean_cols_v3) + 1)] # We add 1 to the end of the range as range() stops before the last number\n","groups_bool_vars"]},{"cell_type":"code","execution_count":39,"id":"22ebc0c6","metadata":{},"outputs":[{"data":{"text/plain":["4"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["df_v3['orientation'].nunique(dropna = True) # Count does not include NaN"]},{"cell_type":"markdown","id":"98583ec4","metadata":{},"source":["Then, we can create the groups for categorical variables as follows (note how below we don't add 1 since we will have n-1 dummies):"]},{"cell_type":"code","execution_count":44,"id":"ef1c9f43","metadata":{},"outputs":[{"data":{"text/plain":["[1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["groups_categorical_vars = [1 for x in range(1, df_v3[categorical_cols_v3[0]].nunique(dropna = True))] + [2 for x in range(1, df_v3[categorical_cols_v3[1]].nunique(dropna = True))]\n","groups_categorical_vars"]},{"cell_type":"markdown","id":"740ed949","metadata":{},"source":["To increase readability, we can convert this into a loop:"]},{"cell_type":"code","execution_count":50,"id":"bf75b15a","metadata":{},"outputs":[{"data":{"text/plain":["[1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3]"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["groups_categorical_vars = []\n","# For every categorical variable in the list (i)\n","for i in range(0, len(categorical_cols_v3)):\n","    # For every number of unique categories (-1) in the categorical variable\n","    for j in range(1, df_v3[categorical_cols_v3[i]].nunique(dropna = True)):\n","        groups_categorical_vars.append(i + 1)\n","\n","groups_categorical_vars"]},{"cell_type":"markdown","id":"f94e7500","metadata":{},"source":["Finally, we should define the group so that one list builds on top of the other, with numerical variables starting the count, followed by booleans and then by categorical variables:"]},{"cell_type":"code","execution_count":78,"id":"49457177","metadata":{},"outputs":[{"data":{"text/plain":["[1,\n"," 2,\n"," 3,\n"," 4,\n"," 5,\n"," 6,\n"," 7,\n"," 8,\n"," 9,\n"," 10,\n"," 10,\n"," 10,\n"," 11,\n"," 11,\n"," 11,\n"," 11,\n"," 11,\n"," 11,\n"," 11,\n"," 11,\n"," 11,\n"," 12,\n"," 12,\n"," 12,\n"," 12,\n"," 12,\n"," 12,\n"," 12,\n"," 12,\n"," 12]"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["# First, we initialize the list with the groups for numerical variables\n","groups = [x for x in range(1, len(numerical_cols_v3) + 1)]\n","\n","# Then, we add to the groups list the boolean variables\n","groups = groups + [x + len(groups) for x in range(1, len(boolean_cols_v3) + 1)]\n","\n","# Finally, we add to the groups list the categorical variables\n","# We save the initial length of the groups list in each iteration here, so that \n","# the length does not change when we add one more for each category below\n","start_cat_index = len(groups) + 1\n","for i, col in enumerate(categorical_cols_v3):\n","    unique_count = df_v3[col].nunique(dropna=True)  # Get the number of unique categories\n","    groups.extend([start_cat_index + i] * (unique_count - 1))  # Assign the same group ID to each unique category\n","\n","groups"]},{"cell_type":"markdown","id":"2b5a69b6","metadata":{},"source":["This works as expected!"]},{"cell_type":"markdown","id":"97b33978","metadata":{},"source":["#### E. Checking that the preprocessor pipeline works together with GroupLasso"]},{"cell_type":"code","execution_count":90,"id":"4c62f5f3","metadata":{},"outputs":[],"source":["# Create transformers for categorical and numerical features\n","numerical_transformer = [\n","    ('scaler', StandardScaler())  # Standardize numerical features\n","    ]\n","    \n","categorical_transformer = [\n","    ('onehot', OneHotEncoder(handle_unknown='error', drop = 'first'))  # One-hot encode categorical features, dropping first dummy\n","]\n","\n","my_n_neighbors = 5\n","\n","# Add KNN imputer for numerical features. We append it, so that standardization is\n","# performed before KNN Imputing\n","numerical_transformer.append(('knn_imputer', KNNImputer(n_neighbors = my_n_neighbors)))\n","# Optionally, use SimpleImputer for categorical features instead of KNN. We insert it \n","# BEFORE one-hot encoding the categorical variables\n","categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","\n","# Create the ColumnTransformer to apply different transformations to numerical\n","# columns, categorical and dummy (boolean) columns\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', Pipeline(numerical_transformer), numerical_cols_v3),\n","        ('cat', Pipeline(categorical_transformer), categorical_cols_v3),\n","        ('bool', SimpleImputer(strategy='most_frequent'), boolean_cols_v3)\n","    ]\n",")\n","\n","# Create steps of the pipeline\n","steps = [('preprocessor', preprocessor)]\n","\n","###############################################################################\n","\n","# Creating the groups for GroupLasso\n","\n","# First, we initialize the list with the groups for numerical variables\n","lasso_groups = [x for x in range(1, len(numerical_cols_v3) + 1)]\n","\n","# Then, we add to the groups list the boolean variables\n","lasso_groups = lasso_groups + [x + len(lasso_groups) for x in range(1, len(boolean_cols_v3) + 1)]\n","\n","# Finally, we add to the groups list the categorical variables\n","# We save the initial length of the groups list in each iteration here, so that \n","# the length does not change when we add one more for each category below\n","start_cat_index = len(lasso_groups) + 1\n","for i, col in enumerate(categorical_cols_v3):\n","    unique_count = df_v3[col].nunique(dropna=True)  # Get the number of unique categories\n","    lasso_groups.extend([start_cat_index + i] * (unique_count - 1))  # Assign the same group ID to each unique category\n","\n","###############################################################################\n","\n","steps.append(('group_lasso', \n","                        GroupLasso(\n","                            groups = lasso_groups, \n","                            group_reg = 0.05, \n","                            l1_reg = 0.05, \n","                            fit_intercept = True, \n","                            n_iter = 500,\n","                            scale_reg = \"group_size\",\n","                            supress_warning = True\n","                            )))\n","            \n","# We create the pipeline with the steps specified above\n","pipeline = Pipeline(steps)\n","\n","###############################################################################\n","\n","# We perform cross-validation\n","\n","target = 'price'\n","n_folds = 5\n","\n","cv_results = cross_validate(\n","                estimator = pipeline, \n","                X = df_v3[features_v3], y = df_v3[target], cv = n_folds, \n","                scoring='neg_mean_squared_error',\n","                return_estimator=True\n","                )"]},{"cell_type":"markdown","id":"d670c043","metadata":{},"source":["***Methods to Validate Code Execution and Results***\n","\n","To ensure the code performs as expected, try the following:\n","\n","1. **Verify Preprocessing Steps**  \n","   - **Inspect Dummy Variables**: After cross-validation, check the structure of dummy variables to confirm that the first category has been consistently dropped. You can retrieve the first estimator’s pipeline and output transformed columns, as in: "]},{"cell_type":"code","execution_count":87,"id":"e5d0690f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Transformed categorical features: ['orientation_north' 'orientation_south' 'orientation_west'\n"," 'neighborhood_Eixample' 'neighborhood_Gràcia' 'neighborhood_Horta'\n"," 'neighborhood_Les Cors' 'neighborhood_Nou Barris'\n"," 'neighborhood_Sant Andreu' 'neighborhood_Sant Martí' 'neighborhood_Sants'\n"," 'neighborhood_Sarrià-Sant Gervasi' 'floor_10' 'floor_2' 'floor_3'\n"," 'floor_4' 'floor_5' 'floor_6' 'floor_7' 'floor_8' 'floor_9']\n"]}],"source":["# Retrieve one pipeline for inspection\n","first_estimator = cv_results['estimator'][0]\n","transformed_data = first_estimator.named_steps['preprocessor'].transform(df_v3[features_v3])\n","\n","# Check the columns for expected transformations\n","categorical_transformed_names = first_estimator.named_steps['preprocessor'] \\\n","                                .named_transformers_['cat'] \\\n","                                .named_steps['onehot'] \\\n","                                .get_feature_names_out(categorical_cols_v3)\n","\n","print(\"Transformed categorical features:\", categorical_transformed_names)"]},{"cell_type":"markdown","id":"a11e7e81","metadata":{},"source":["The first category has been dropped!"]},{"cell_type":"markdown","id":"e02c45a9","metadata":{},"source":["2. **Validate Grouping for Group Lasso**\n","   - Print `lasso_groups` to verify that it matches the expected format and group counts for numerical, boolean, and categorical columns:\n","   - Ensure that the length of `lasso_groups` aligns with the total number of columns post-transformation."]},{"cell_type":"code","execution_count":88,"id":"f1ef3f1d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Lasso Groups: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n"]}],"source":["print(\"Lasso Groups:\", lasso_groups)"]},{"cell_type":"markdown","id":"40c210c6","metadata":{},"source":["Groups match the length of columns post-transformation!"]},{"cell_type":"markdown","id":"c1962a49","metadata":{},"source":["3. **Cross-Validation Results Check**\n","   - Inspect the `cv_results` to check both the mean cross-validated score and individual fold scores:\n","   - If possible, validate the shape and types of `cv_results['estimator']` to confirm that each step executed without error across all folds."]},{"cell_type":"code","execution_count":91,"id":"1e840846","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean CV score: -31067.83582374016, Std Dev of CV score: 937.7856589639939\n"]}],"source":["# Mean and std deviation of cross-validated negative MSE scores\n","mean_score = np.mean(cv_results['test_score'])\n","std_score = np.std(cv_results['test_score'])\n","print(f\"Mean CV score: {mean_score}, Std Dev of CV score: {std_score}\")"]},{"cell_type":"markdown","id":"937b401a","metadata":{},"source":["4. **Final Validation by Output Inspection**  \n","   - Check the `transformed_data` shape matches the expected column count (number of numerical + number of one-hot encoded + number of boolean).\n","   - Optionally, apply `pipeline.fit(df_v3[features_v3], df_v3[target])` to the entire dataset and compare predictions or transformed features against known results or expectations, ensuring the pipeline works outside of cross-validation.\n","\n","These checks should give you confidence that the pipeline and cross-validation workflow operate as expected and handle preprocessing and grouping correctly across all stages."]},{"cell_type":"markdown","id":"c7d4f812","metadata":{},"source":["Everything seems good to go!! Now, everything can be joined in the function."]},{"cell_type":"markdown","id":"46da060a","metadata":{},"source":["### Definition of the function"]},{"cell_type":"code","execution_count":15,"id":"e0e87501","metadata":{},"outputs":[],"source":["def group_lasso_imputers(\n","        df: pd.DataFrame, numerical_features: list[str], boolean_features: list[str], \n","        categorical_features: list[str],target: str, n_folds: int = 5, \n","        use_imputers: bool = False, my_n_neighbors: int = 5, max_n_lasso_iter: int = 100\n","        ):\n","\n","    ###########################################################################\n","\n","    results_list = []  # To store all results we want for our data frame\n","    alpha_counter = 0  # Initialize a counter for the alphas\n","    group_reg_parameter_counter = 0 # Initialize a counter for the group regularization parameter\n","    features = numerical_features + boolean_features + categorical_features\n","\n","    ###########################################################################\n","\n","    # Create transformers for categorical and numerical features\n","    numerical_transformer = [('scaler', StandardScaler())]  # Standardize numerical features\n","        \n","    categorical_transformer = [('onehot', OneHotEncoder(handle_unknown='error', drop = 'first'))]  # One-hot encode categorical features, dropping first dummy\n","\n","    if use_imputers == True:\n","        # Add KNN imputer for numerical features. We append it, so that standardization is\n","        # performed before KNN Imputing\n","        numerical_transformer.append(('knn_imputer', KNNImputer(n_neighbors = my_n_neighbors)))\n","        # Optionally, use SimpleImputer for categorical features instead of KNN. We insert it \n","        # BEFORE one-hot encoding the categorical variables\n","        categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","\n","    # Set the boolean transformer based on use_imputers\n","    if use_imputers == True:\n","        # Optionally, use SimpleImputer for boolean features\n","        boolean_transformer = [('simple_imputer', SimpleImputer(strategy='most_frequent'))]\n","    else:\n","        # Pass through boolean features without transformation\n","        boolean_transformer = [('passthrough', 'passthrough')]\n","\n","    # Create the ColumnTransformer to apply different transformations to numerical\n","    # columns, categorical and dummy (boolean) columns\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', Pipeline(numerical_transformer), numerical_features),\n","            ('cat', Pipeline(categorical_transformer), categorical_features),\n","            ('bool', Pipeline(boolean_transformer), boolean_features)\n","        ]\n","    )\n","\n","    # Create steps of the pipeline, which begins with the preprocessor. It will\n","    # apply the transformation and imputations (if use_imputers is set to True) \n","    # to the different features in the order specified in ColumnTransformer\n","    steps = [('preprocessor', preprocessor)]\n","    \n","    ###########################################################################\n","\n","    # We create a list with the definition of the groups for GroupLasso\n","\n","    # First, we initialize the list with the groups for numerical variables\n","    lasso_groups = [x for x in range(1, len(numerical_features) + 1)]\n","\n","    # Then, we add to the groups list the boolean variables\n","    lasso_groups = lasso_groups + [x + len(lasso_groups) for x in range(1, len(boolean_features) + 1)]\n","\n","    # Finally, we add to the groups list the categorical variables\n","    # We save the initial length of the groups list in each iteration here, so that \n","    # the length does not change when we add one more for each category below\n","    start_cat_index = len(lasso_groups) + 1\n","    for i, col in enumerate(categorical_features):\n","        unique_count = df[col].nunique(dropna=True)  # Get the number of unique categories, ignoring NaN\n","        lasso_groups.extend([start_cat_index + i] * (unique_count - 1))  # Assign the same group ID to each unique category \n","\n","    ###########################################################################\n","\n","    # Now, we create a first loop that iterates over different values of the alpha\n","    # parameter. We generate 50 alphas\n","    for alpha in np.logspace(-10, 0, 50):\n","    \n","        # We add 1 to the counter of alpha each time we consider another alpha\n","        alpha_counter += 1\n","\n","        # We reset the counter of the group_parameter after the nested loop ends\n","        group_reg_parameter_counter = 0\n","\n","        # We create a second loop that iterates over different values of the \n","        # parameter that penalizes group sparsity\n","        for group_parameter in np.logspace(-10, 0, 50):\n","            \n","            # We add 1 to the counter of the group regularization parameter in each iteration\n","            group_reg_parameter_counter += 1\n","\n","            # We add Group Lasso to the pipeline, with a specific alpha\n","            steps.append(('group_lasso', \n","                        GroupLasso(\n","                            groups = lasso_groups, \n","                            group_reg = group_parameter, \n","                            l1_reg = alpha, \n","                            fit_intercept = True, \n","                            n_iter = max_n_lasso_iter,\n","                            scale_reg = \"group_size\",\n","                            supress_warning = True\n","                            )))\n","            \n","            # We create the pipeline with the steps specified above\n","            pipeline = Pipeline(steps)\n","\n","            # Perform cross-validation: on each fold, it will scale and fit the training\n","            # data, and compute the negative mean squared error (so a higher value, i.e.,\n","            # more negative, is better).\n","\n","            cv_results = cross_validate(\n","                estimator = pipeline, \n","                X = df[features], \n","                y = df[target], cv = n_folds, scoring = 'neg_mean_squared_error',\n","                return_estimator = True\n","                )\n","\n","            neg_mse = cv_results['test_score']  # Negative MSE scores\n","            estimators = cv_results['estimator']  # List of fitted estimators for each fold\n","            \n","            # Loop through each fold result to store the information\n","            for index, (score, estimator) in enumerate(zip(neg_mse, estimators)):\n","                # We access the information associated to the group lasso\n","                group_lasso = estimator.named_steps['group_lasso']\n","                # We store the coefficients estimated by group lasso\n","                coefs = group_lasso.coef_\n","                # We store the intercept estimated by group lasso\n","                intercept = group_lasso.intercept_[0]\n","\n","                # Create a dictionary to store the results for this fold\n","                result_dict = {\n","                    'Fold': index + 1,\n","                    'Alpha_counter': alpha_counter,\n","                    'Group_reg_parameter_counter': group_reg_parameter_counter,\n","                    'Alpha': alpha,\n","                    'Group_reg_parameter': group_parameter,\n","                    'Negative MSE': score,\n","                    'Intercept': intercept\n","                }\n","\n","                # Get feature names after one-hot encoding\n","                cat_feature_names = estimator.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n","                combined_features = np.concatenate([numerical_features, boolean_features, cat_feature_names])\n","\n","                # Assign coefficients to the corresponding features\n","                for i, feature in enumerate(combined_features):\n","                    result_dict[feature] = coefs[i][0]\n","\n","                # Append the result to the list\n","                results_list.append(result_dict)\n","    \n","            # Lastly, we remove the last step of the pipeline so that the \n","            # GroupLasso step does not accumulate with different hyperparameters.\n","            # We do this for each iteration of group_reg\n","            steps.pop()\n","\n","    # Convert the results list to a DataFrame\n","    results_df = pd.DataFrame(results_list)\n","\n","    return results_df"]},{"cell_type":"markdown","id":"fcae428d","metadata":{},"source":["### Training with function 4"]},{"cell_type":"markdown","id":"8d564621","metadata":{},"source":["#### Test"]},{"cell_type":"code","execution_count":11,"id":"f1bb2bb9","metadata":{},"outputs":[],"source":["# Test with function 4\n","# results_v3_f4_test = group_lasso_imputers(df = df_v3, numerical_features = ['square_meters'], boolean_features = ['has_ac'], categorical_features = ['orientation'], target = 'price', n_folds = 5, use_imputers = True, my_n_neighbors = 5, max_n_lasso_iter = 500)\n","\n","# results_v3_f4_test.to_csv('3_results_datasets/results_v3_f4_test.csv')"]},{"cell_type":"markdown","id":"f10c9353","metadata":{},"source":["This test has lasted for 44 minutes (with only one numerical, one dummy and one categorical variable!). Changes to apply: reset counter `group_reg_parameter_counter` after all of the combinations have been tried (maybe define it as equal to 0 in the first loop)."]},{"cell_type":"code","execution_count":null,"id":"88cca683","metadata":{},"outputs":[],"source":["# results_v3_f4_test"]},{"cell_type":"markdown","id":"b951aac3","metadata":{},"source":["#### Training with all of the features"]},{"cell_type":"code","execution_count":16,"id":"a73643fc","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n","/home/pablo/miniconda3/envs/py312/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n","You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n","  warnings.warn(\n"]}],"source":["# results_v3_f4 = group_lasso_imputers(df = df_v3, numerical_features = numerical_cols_v3, boolean_features = boolean_cols_v3, categorical_features = categorical_cols_v3, target = 'price', n_folds = 5, use_imputers = True, my_n_neighbors = 5, max_n_lasso_iter = 500)\n","# results_v3_f4.to_csv('3_results_datasets/results_v3_f4.csv')"]},{"cell_type":"markdown","id":"1a5a2134","metadata":{},"source":["It has been 160 minutes doing the computations, and in approx. 15 folds it did not converge."]},{"cell_type":"code","execution_count":null,"id":"ac8e31ea","metadata":{},"outputs":[],"source":["# results_v3_f4"]},{"cell_type":"markdown","id":"19dd0f1d","metadata":{},"source":["## Function 5: function 4 but with regular linear regression instead and trying all possible combinations (treating dummies as a unit)\n","The idea here would be to impute null values in the same way as in function 4, but instead try all possible combinations of linear regression as implemented in function 1. This function, `lin_reg_with_preprocessing_combos`, performs linear regression with cross-validation on all possible combinations of features (numerical, boolean, and categorical) in a dataset, while dynamically applying preprocessing steps based on the types of features present in each combination. Key steps:\n","\n","1. **Feature Combinations:** It generates all possible subsets of features (combinations of numerical, boolean, and categorical) to test various models with different feature groups.\n","  \n","2. **Dynamic Preprocessing:** \n","   - For each combination, it builds a `ColumnTransformer` that includes preprocessing steps only for the types of features present:\n","     - **Numerical Features**: Standard scaling and optional KNN imputation.\n","     - **Categorical Features**: One-hot encoding (dropping the first category) and optional most-frequent imputation.\n","     - **Boolean Features**: Optional most-frequent imputation or passthrough.\n","     \n","3. **Pipeline and Cross-Validation:** \n","   - For each feature subset, it creates a pipeline with the `ColumnTransformer` and a linear regression model.\n","   - Cross-validation is performed on the pipeline, returning the negative mean squared error (MSE) and fitted model for each fold.\n","\n","4. **Result Storage and Output:**\n","   - For each fold and combination, it records:\n","     - Model performance (negative MSE).\n","     - Model coefficients and intercept.\n","     - Feature coefficients are mapped to feature names, and features not in the combination are assigned `NaN`.\n","   - Results for all combinations and folds are stored in a DataFrame and returned."]},{"cell_type":"code","execution_count":13,"id":"8fb74cea","metadata":{},"outputs":[],"source":["def lin_reg_with_preprocessing_combos(\n","    df: pd.DataFrame, numerical_features: list[str], boolean_features: list[str], \n","    categorical_features: list[str], target: str, n_folds: int = 5, use_imputers: bool = False,\n","    my_n_neighbors: int = 5\n","):\n","\n","    def flatten_combo(combo):\n","        \"\"\"\n","        Flattens the combination by including all variables from nested lists\n","        when one element of a list is selected.\n","        \"\"\"\n","        return list(itertools.chain(*[x if isinstance(x, list) else [x] for x in combo]))\n","\n","    results_list = []  # To store all results we want for our data frame\n","    combo_counter = 0  # Initialize a counter for the combinations\n","    features = numerical_features + boolean_features + categorical_features\n","\n","    for r in range(1, len(features) + 1):\n","        for combo in itertools.combinations(features, r):\n","\n","            combo_counter += 1\n","\n","            # We flatten the list of the resulting combination of features\n","            flat_combo = flatten_combo(combo)\n","\n","            # Determine which types of features are present in the current combination\n","            current_numerical_features = [f for f in flat_combo if f in numerical_features]\n","            current_boolean_features = [f for f in flat_combo if f in boolean_features]\n","            current_categorical_features = [f for f in flat_combo if f in categorical_features]\n","\n","            # Build the transformers dynamically based on the present feature types\n","            transformers = []\n","            \n","            # Add numerical transformer if numerical features are present\n","            if current_numerical_features:\n","                numerical_transformer = [('scaler', StandardScaler())]\n","                if use_imputers:\n","                    numerical_transformer.append(('knn_imputer', KNNImputer(n_neighbors=my_n_neighbors)))\n","                transformers.append(('num', Pipeline(numerical_transformer), current_numerical_features))\n","                \n","            # Add categorical transformer if categorical features are present\n","            if current_categorical_features:\n","                categorical_transformer = [('onehot', OneHotEncoder(handle_unknown='error', drop='first'))]\n","                if use_imputers:\n","                    categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","                transformers.append(('cat', Pipeline(categorical_transformer), current_categorical_features))\n","                \n","            # Add boolean transformer if boolean features are present\n","            if current_boolean_features:\n","                if use_imputers:\n","                    boolean_transformer = [('simple_imputer', SimpleImputer(strategy='most_frequent'))]\n","                else:\n","                    boolean_transformer = [('passthrough', 'passthrough')]\n","                transformers.append(('bool', Pipeline(boolean_transformer), current_boolean_features))\n","                \n","            # Define the preprocessor with only the applicable transformers\n","            preprocessor = ColumnTransformer(transformers=transformers)\n","\n","            # Pipeline: apply transformations and linear regression\n","            pipeline = Pipeline([\n","                ('preprocessor', preprocessor),\n","                ('linreg', LinearRegression(fit_intercept=True))\n","            ])\n","\n","            # Perform cross-validation and get negative MSE scores\n","            cv_results = cross_validate(\n","                estimator=pipeline,\n","                X=df[flat_combo],\n","                y=df[target],\n","                cv=n_folds,\n","                scoring='neg_mean_squared_error',\n","                return_estimator=True\n","            )\n","\n","            neg_mse = cv_results['test_score']\n","            estimators = cv_results['estimator']\n","\n","            # Store results for each fold\n","            for index, (score, estimator) in enumerate(zip(neg_mse, estimators)):\n","                linreg = estimator.named_steps['linreg']\n","                coefs = linreg.coef_\n","                intercept = linreg.intercept_\n","\n","                result_dict = {\n","                    'Fold': index + 1,\n","                    'Combination ID': combo_counter,\n","                    'Negative MSE': score,\n","                    'Intercept': intercept\n","                }\n","\n","                # Get feature names post one-hot encoding and exclude original categorical columns\n","                if current_categorical_features:\n","                    cat_feature_names = estimator.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(current_categorical_features)\n","                else:\n","                    cat_feature_names = []\n","\n","                # Combine the feature names, excluding the original categorical columns\n","                combined_features = np.concatenate([current_numerical_features, current_boolean_features, cat_feature_names])\n","\n","                # Assign coefficients to the corresponding features\n","                for i, feature in enumerate(combined_features):\n","                    result_dict[feature] = coefs[i]\n","\n","                # Assign NaN for features not in the current combination, excluding original categorical columns\n","                for feature in features:\n","                    if feature in numerical_features or feature in boolean_features:\n","                        if feature not in combined_features:\n","                            result_dict[feature] = float('nan')\n","\n","                results_list.append(result_dict)\n","\n","    # Convert results to DataFrame\n","    results_df = pd.DataFrame(results_list)\n","    return results_df"]},{"cell_type":"markdown","id":"6e07d935","metadata":{},"source":["### Training with function 5\n","Now, we apply the function to dataset 3."]},{"cell_type":"code","execution_count":15,"id":"7f5a076d","metadata":{},"outputs":[],"source":["# results_v3_f5 = lin_reg_with_preprocessing_combos(\n","    # df = df_v3, numerical_features = numerical_cols_v3, boolean_features = boolean_cols_v3,\n","    # categorical_features = categorical_cols_v3, target = 'price', n_folds = 5,\n","    # use_imputers = True, my_n_neighbors = 5\n","# )\n","\n","# results_v3_f5.to_csv('3_results_datasets/results_v3_f5.csv')"]},{"cell_type":"code","execution_count":12,"id":"a2f9e45f","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fold</th>\n","      <th>Combination ID</th>\n","      <th>Negative MSE</th>\n","      <th>Intercept</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>num_crimes</th>\n","      <th>is_furnished</th>\n","      <th>...</th>\n","      <th>neighborhood_Sarrià-Sant Gervasi</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>-72989.051984</td>\n","      <td>1099.686250</td>\n","      <td>5.927694</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>-70145.790556</td>\n","      <td>1101.458125</td>\n","      <td>11.108159</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>-75719.267630</td>\n","      <td>1097.982500</td>\n","      <td>7.062621</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>-74018.007181</td>\n","      <td>1099.884531</td>\n","      <td>9.092078</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>-75968.024362</td>\n","      <td>1099.069844</td>\n","      <td>8.212737</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20470</th>\n","      <td>1</td>\n","      <td>4095</td>\n","      <td>-32586.625413</td>\n","      <td>1058.615650</td>\n","      <td>3.917668</td>\n","      <td>7.126842</td>\n","      <td>192.101034</td>\n","      <td>-0.757726</td>\n","      <td>-75.904084</td>\n","      <td>-8.141981</td>\n","      <td>...</td>\n","      <td>33.428130</td>\n","      <td>44.272879</td>\n","      <td>56.444276</td>\n","      <td>57.885396</td>\n","      <td>73.404529</td>\n","      <td>76.642061</td>\n","      <td>2.013203</td>\n","      <td>3.776742</td>\n","      <td>10.158322</td>\n","      <td>-2.401222</td>\n","    </tr>\n","    <tr>\n","      <th>20471</th>\n","      <td>2</td>\n","      <td>4095</td>\n","      <td>-31189.621399</td>\n","      <td>1053.414410</td>\n","      <td>4.476038</td>\n","      <td>7.996682</td>\n","      <td>193.742519</td>\n","      <td>0.812930</td>\n","      <td>-75.533129</td>\n","      <td>-2.849968</td>\n","      <td>...</td>\n","      <td>35.992292</td>\n","      <td>36.003038</td>\n","      <td>52.651858</td>\n","      <td>66.409417</td>\n","      <td>75.126816</td>\n","      <td>74.616775</td>\n","      <td>4.049733</td>\n","      <td>6.425065</td>\n","      <td>4.969810</td>\n","      <td>-1.908622</td>\n","    </tr>\n","    <tr>\n","      <th>20472</th>\n","      <td>3</td>\n","      <td>4095</td>\n","      <td>-29805.370984</td>\n","      <td>1047.106412</td>\n","      <td>4.916282</td>\n","      <td>6.317971</td>\n","      <td>188.796832</td>\n","      <td>-1.523096</td>\n","      <td>-76.388400</td>\n","      <td>-1.363997</td>\n","      <td>...</td>\n","      <td>37.926515</td>\n","      <td>45.501094</td>\n","      <td>61.977180</td>\n","      <td>66.666239</td>\n","      <td>69.623761</td>\n","      <td>78.723152</td>\n","      <td>1.894611</td>\n","      <td>6.227323</td>\n","      <td>5.901172</td>\n","      <td>-0.808511</td>\n","    </tr>\n","    <tr>\n","      <th>20473</th>\n","      <td>4</td>\n","      <td>4095</td>\n","      <td>-30412.817955</td>\n","      <td>1044.818466</td>\n","      <td>4.711976</td>\n","      <td>9.492917</td>\n","      <td>189.692961</td>\n","      <td>-0.766892</td>\n","      <td>-76.860957</td>\n","      <td>1.470256</td>\n","      <td>...</td>\n","      <td>26.221743</td>\n","      <td>38.361906</td>\n","      <td>57.378565</td>\n","      <td>60.068034</td>\n","      <td>70.781013</td>\n","      <td>75.278541</td>\n","      <td>6.783332</td>\n","      <td>5.619868</td>\n","      <td>8.575298</td>\n","      <td>2.863775</td>\n","    </tr>\n","    <tr>\n","      <th>20474</th>\n","      <td>5</td>\n","      <td>4095</td>\n","      <td>-31405.396078</td>\n","      <td>1049.849450</td>\n","      <td>3.797850</td>\n","      <td>5.971341</td>\n","      <td>189.547885</td>\n","      <td>-0.095415</td>\n","      <td>-75.176167</td>\n","      <td>-3.414160</td>\n","      <td>...</td>\n","      <td>24.830778</td>\n","      <td>33.015063</td>\n","      <td>49.250481</td>\n","      <td>64.616720</td>\n","      <td>62.177033</td>\n","      <td>71.770241</td>\n","      <td>2.165158</td>\n","      <td>7.505489</td>\n","      <td>8.875008</td>\n","      <td>0.785281</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20475 rows × 37 columns</p>\n","</div>"],"text/plain":["       Fold  Combination ID  Negative MSE    Intercept  num_rooms  num_baths  \\\n","0         1               1 -72989.051984  1099.686250   5.927694        NaN   \n","1         2               1 -70145.790556  1101.458125  11.108159        NaN   \n","2         3               1 -75719.267630  1097.982500   7.062621        NaN   \n","3         4               1 -74018.007181  1099.884531   9.092078        NaN   \n","4         5               1 -75968.024362  1099.069844   8.212737        NaN   \n","...     ...             ...           ...          ...        ...        ...   \n","20470     1            4095 -32586.625413  1058.615650   3.917668   7.126842   \n","20471     2            4095 -31189.621399  1053.414410   4.476038   7.996682   \n","20472     3            4095 -29805.370984  1047.106412   4.916282   6.317971   \n","20473     4            4095 -30412.817955  1044.818466   4.711976   9.492917   \n","20474     5            4095 -31405.396078  1049.849450   3.797850   5.971341   \n","\n","       square_meters  year_built  num_crimes  is_furnished  ...  \\\n","0                NaN         NaN         NaN           NaN  ...   \n","1                NaN         NaN         NaN           NaN  ...   \n","2                NaN         NaN         NaN           NaN  ...   \n","3                NaN         NaN         NaN           NaN  ...   \n","4                NaN         NaN         NaN           NaN  ...   \n","...              ...         ...         ...           ...  ...   \n","20470     192.101034   -0.757726  -75.904084     -8.141981  ...   \n","20471     193.742519    0.812930  -75.533129     -2.849968  ...   \n","20472     188.796832   -1.523096  -76.388400     -1.363997  ...   \n","20473     189.692961   -0.766892  -76.860957      1.470256  ...   \n","20474     189.547885   -0.095415  -75.176167     -3.414160  ...   \n","\n","       neighborhood_Sarrià-Sant Gervasi   floor_10    floor_2    floor_3  \\\n","0                                   NaN        NaN        NaN        NaN   \n","1                                   NaN        NaN        NaN        NaN   \n","2                                   NaN        NaN        NaN        NaN   \n","3                                   NaN        NaN        NaN        NaN   \n","4                                   NaN        NaN        NaN        NaN   \n","...                                 ...        ...        ...        ...   \n","20470                         33.428130  44.272879  56.444276  57.885396   \n","20471                         35.992292  36.003038  52.651858  66.409417   \n","20472                         37.926515  45.501094  61.977180  66.666239   \n","20473                         26.221743  38.361906  57.378565  60.068034   \n","20474                         24.830778  33.015063  49.250481  64.616720   \n","\n","         floor_4    floor_5   floor_6   floor_7    floor_8   floor_9  \n","0            NaN        NaN       NaN       NaN        NaN       NaN  \n","1            NaN        NaN       NaN       NaN        NaN       NaN  \n","2            NaN        NaN       NaN       NaN        NaN       NaN  \n","3            NaN        NaN       NaN       NaN        NaN       NaN  \n","4            NaN        NaN       NaN       NaN        NaN       NaN  \n","...          ...        ...       ...       ...        ...       ...  \n","20470  73.404529  76.642061  2.013203  3.776742  10.158322 -2.401222  \n","20471  75.126816  74.616775  4.049733  6.425065   4.969810 -1.908622  \n","20472  69.623761  78.723152  1.894611  6.227323   5.901172 -0.808511  \n","20473  70.781013  75.278541  6.783332  5.619868   8.575298  2.863775  \n","20474  62.177033  71.770241  2.165158  7.505489   8.875008  0.785281  \n","\n","[20475 rows x 37 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# results_v3_f5"]},{"cell_type":"markdown","id":"cc72f1d2","metadata":{},"source":["## Function 6: function 4 but with regular Lasso instead"]},{"cell_type":"code","execution_count":47,"id":"128591da","metadata":{},"outputs":[],"source":["def lasso_imputers(\n","        df: pd.DataFrame, numerical_features: list[str], boolean_features: list[str], \n","        categorical_features: list[str],target: str, n_folds: int = 5, \n","        use_imputers: bool = False, my_n_neighbors: int = 5, max_n_lasso_iter: int = 100\n","        ):\n","\n","    ###########################################################################\n","\n","    results_list = []  # To store all results we want for our data frame\n","    alpha_counter = 0  # Initialize a counter for the alphas\n","    features = numerical_features + boolean_features + categorical_features\n","\n","    ###########################################################################\n","\n","    # Create transformers for categorical and numerical features\n","    numerical_transformer = [('scaler', StandardScaler())]  # Standardize numerical features\n","        \n","    categorical_transformer = [('onehot', OneHotEncoder(handle_unknown='error', drop = 'first'))]  # One-hot encode categorical features, dropping first dummy\n","\n","    if use_imputers == True:\n","        # Add KNN imputer for numerical features. We append it, so that standardization is\n","        # performed before KNN Imputing\n","        numerical_transformer.append(('knn_imputer', KNNImputer(n_neighbors = my_n_neighbors)))\n","        # Optionally, use SimpleImputer for categorical features instead of KNN. We insert it \n","        # BEFORE one-hot encoding the categorical variables\n","        categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","\n","    # Set the boolean transformer based on use_imputers\n","    if use_imputers == True:\n","        # Optionally, use SimpleImputer for boolean features\n","        boolean_transformer = [('simple_imputer', SimpleImputer(strategy='most_frequent'))]\n","    else:\n","        # Pass through boolean features without transformation\n","        boolean_transformer = [('passthrough', 'passthrough')]\n","\n","    # Create the ColumnTransformer to apply different transformations to numerical\n","    # columns, categorical and dummy (boolean) columns\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', Pipeline(numerical_transformer), numerical_features),\n","            ('cat', Pipeline(categorical_transformer), categorical_features),\n","            ('bool', Pipeline(boolean_transformer), boolean_features)\n","        ]\n","    )\n","\n","    # Create steps of the pipeline, which begins with the preprocessor. It will\n","    # apply the transformation and imputations (if use_imputers is set to True) \n","    # to the different features in the order specified in ColumnTransformer\n","    steps = [('preprocessor', preprocessor)]\n","    \n","    ###########################################################################\n","\n","    # Now, we create a loop that iterates over different values of the alpha\n","    # parameter. We generate 50 alphas\n","    for alpha in np.logspace(-10, 0, 50):\n","    \n","        # We add 1 to the counter of alpha each time we consider another alpha\n","        alpha_counter += 1\n","\n","        # We add Lasso to the pipeline, with a specific alpha\n","        steps.append(('lasso', \n","                    Lasso(alpha = alpha,\n","                          fit_intercept = True,\n","                          max_iter = max_n_lasso_iter\n","                        )))\n","        \n","        # We create the pipeline with the steps specified above\n","        pipeline = Pipeline(steps)\n","\n","        # Perform cross-validation: on each fold, it will scale and fit the training\n","        # data, and compute the negative mean squared error (so a higher value, i.e.,\n","        # more negative, is better).\n","\n","        cv_results = cross_validate(\n","            estimator = pipeline, \n","            X = df[features], \n","            y = df[target], cv = n_folds, scoring = 'neg_mean_squared_error',\n","            return_estimator = True\n","            )\n","\n","        neg_mse = cv_results['test_score']  # Negative MSE scores\n","        estimators = cv_results['estimator']  # List of fitted estimators for each fold\n","        \n","        # Loop through each fold result to store the information\n","        for index, (score, estimator) in enumerate(zip(neg_mse, estimators)):\n","            # We access the information associated to the lasso\n","            lasso = estimator.named_steps['lasso']\n","            # We store the coefficients estimated by lasso\n","            coefs = lasso.coef_\n","            # We store the intercept estimated by lasso\n","            intercept = lasso.intercept_\n","\n","            # Create a dictionary to store the results for this fold\n","            result_dict = {\n","                'Fold': index + 1,\n","                'Alpha_counter': alpha_counter,\n","                'Alpha': alpha,\n","                'Negative MSE': score,\n","                'Intercept': intercept\n","            }\n","\n","            # Get feature names after one-hot encoding\n","            cat_feature_names = estimator.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n","            combined_features = np.concatenate([numerical_features, boolean_features, cat_feature_names])\n","\n","            # Assign coefficients to the corresponding features\n","            for i, feature in enumerate(combined_features):\n","                result_dict[feature] = coefs[i]\n","\n","            # Append the result to the list\n","            results_list.append(result_dict)\n","\n","        # Lastly, we remove the last step of the pipeline so that the \n","        # Lasso step does not accumulate with different hyperparameters.\n","        # We do this for each iteration of alpha\n","        steps.pop()\n","\n","    # Convert the results list to a DataFrame\n","    results_df = pd.DataFrame(results_list)\n","\n","    return results_df"]},{"cell_type":"markdown","id":"0ce335e8","metadata":{},"source":["### Training with function 6"]},{"cell_type":"code","execution_count":49,"id":"fa46b714","metadata":{},"outputs":[],"source":["# results_v3_f6 = lasso_imputers(\n","    # df = df_v3, numerical_features = numerical_cols_v3, boolean_features = boolean_cols_v3, \n","    # categorical_features = categorical_cols_v3, target = 'price', n_folds = 5, use_imputers = True, my_n_neighbors = 5, \n","    # max_n_lasso_iter = 500)\n","\n","# results_v3_f6.to_csv('3_results_datasets/results_v3_f6.csv')"]},{"cell_type":"code","execution_count":25,"id":"6eb9c753","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fold</th>\n","      <th>Alpha_counter</th>\n","      <th>Alpha</th>\n","      <th>Negative MSE</th>\n","      <th>Intercept</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>num_crimes</th>\n","      <th>...</th>\n","      <th>neighborhood_Sarrià-Sant Gervasi</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-32586.625413</td>\n","      <td>1058.615650</td>\n","      <td>3.917668</td>\n","      <td>7.126842</td>\n","      <td>192.101034</td>\n","      <td>-0.757726</td>\n","      <td>-75.904084</td>\n","      <td>...</td>\n","      <td>33.428130</td>\n","      <td>44.272879</td>\n","      <td>56.444276</td>\n","      <td>57.885396</td>\n","      <td>73.404529</td>\n","      <td>76.642061</td>\n","      <td>2.013203</td>\n","      <td>3.776742</td>\n","      <td>10.158322</td>\n","      <td>-2.401222</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-31189.621399</td>\n","      <td>1053.414410</td>\n","      <td>4.476038</td>\n","      <td>7.996682</td>\n","      <td>193.742519</td>\n","      <td>0.812930</td>\n","      <td>-75.533129</td>\n","      <td>...</td>\n","      <td>35.992292</td>\n","      <td>36.003038</td>\n","      <td>52.651858</td>\n","      <td>66.409417</td>\n","      <td>75.126816</td>\n","      <td>74.616775</td>\n","      <td>4.049733</td>\n","      <td>6.425065</td>\n","      <td>4.969810</td>\n","      <td>-1.908622</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-29805.370984</td>\n","      <td>1047.106412</td>\n","      <td>4.916282</td>\n","      <td>6.317971</td>\n","      <td>188.796832</td>\n","      <td>-1.523096</td>\n","      <td>-76.388400</td>\n","      <td>...</td>\n","      <td>37.926515</td>\n","      <td>45.501094</td>\n","      <td>61.977180</td>\n","      <td>66.666239</td>\n","      <td>69.623761</td>\n","      <td>78.723152</td>\n","      <td>1.894611</td>\n","      <td>6.227323</td>\n","      <td>5.901172</td>\n","      <td>-0.808511</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-30412.817955</td>\n","      <td>1044.818466</td>\n","      <td>4.711976</td>\n","      <td>9.492917</td>\n","      <td>189.692961</td>\n","      <td>-0.766892</td>\n","      <td>-76.860957</td>\n","      <td>...</td>\n","      <td>26.221743</td>\n","      <td>38.361906</td>\n","      <td>57.378565</td>\n","      <td>60.068034</td>\n","      <td>70.781013</td>\n","      <td>75.278541</td>\n","      <td>6.783332</td>\n","      <td>5.619868</td>\n","      <td>8.575298</td>\n","      <td>2.863775</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-31405.396078</td>\n","      <td>1049.849450</td>\n","      <td>3.797850</td>\n","      <td>5.971341</td>\n","      <td>189.547885</td>\n","      <td>-0.095415</td>\n","      <td>-75.176167</td>\n","      <td>...</td>\n","      <td>24.830778</td>\n","      <td>33.015063</td>\n","      <td>49.250481</td>\n","      <td>64.616720</td>\n","      <td>62.177033</td>\n","      <td>71.770241</td>\n","      <td>2.165158</td>\n","      <td>7.505489</td>\n","      <td>8.875008</td>\n","      <td>0.785281</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>245</th>\n","      <td>1</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-32604.351129</td>\n","      <td>1088.581253</td>\n","      <td>2.884792</td>\n","      <td>6.277713</td>\n","      <td>191.155709</td>\n","      <td>-0.000000</td>\n","      <td>-75.716975</td>\n","      <td>...</td>\n","      <td>-0.000000</td>\n","      <td>0.000000</td>\n","      <td>9.460519</td>\n","      <td>10.103815</td>\n","      <td>25.372687</td>\n","      <td>30.161026</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>5.730685</td>\n","      <td>-0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>246</th>\n","      <td>2</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-31242.094652</td>\n","      <td>1089.915224</td>\n","      <td>3.381021</td>\n","      <td>7.204187</td>\n","      <td>192.503337</td>\n","      <td>0.000000</td>\n","      <td>-75.412434</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>9.331053</td>\n","      <td>21.120634</td>\n","      <td>29.966812</td>\n","      <td>30.335900</td>\n","      <td>0.000000</td>\n","      <td>2.202022</td>\n","      <td>0.801199</td>\n","      <td>-0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>247</th>\n","      <td>3</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-29881.521004</td>\n","      <td>1089.562456</td>\n","      <td>3.758123</td>\n","      <td>5.507426</td>\n","      <td>187.578657</td>\n","      <td>-0.381861</td>\n","      <td>-75.710976</td>\n","      <td>...</td>\n","      <td>-0.000000</td>\n","      <td>0.000000</td>\n","      <td>12.680365</td>\n","      <td>17.250843</td>\n","      <td>20.419509</td>\n","      <td>29.721570</td>\n","      <td>0.000000</td>\n","      <td>1.628526</td>\n","      <td>1.271786</td>\n","      <td>-0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>248</th>\n","      <td>4</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-30471.671817</td>\n","      <td>1086.952384</td>\n","      <td>3.670738</td>\n","      <td>8.564819</td>\n","      <td>188.503710</td>\n","      <td>-0.000000</td>\n","      <td>-76.605951</td>\n","      <td>...</td>\n","      <td>-0.000000</td>\n","      <td>0.000000</td>\n","      <td>14.390252</td>\n","      <td>16.481862</td>\n","      <td>27.912827</td>\n","      <td>32.294465</td>\n","      <td>1.952138</td>\n","      <td>1.181696</td>\n","      <td>4.143621</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>249</th>\n","      <td>5</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-31657.198702</td>\n","      <td>1085.735722</td>\n","      <td>2.747021</td>\n","      <td>5.059718</td>\n","      <td>188.524121</td>\n","      <td>0.000000</td>\n","      <td>-74.973181</td>\n","      <td>...</td>\n","      <td>-0.000000</td>\n","      <td>0.000000</td>\n","      <td>10.840507</td>\n","      <td>24.078377</td>\n","      <td>21.489972</td>\n","      <td>31.763198</td>\n","      <td>0.000000</td>\n","      <td>3.060523</td>\n","      <td>4.223587</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>250 rows × 35 columns</p>\n","</div>"],"text/plain":["     Fold  Alpha_counter         Alpha  Negative MSE    Intercept  num_rooms  \\\n","0       1              1  1.000000e-10 -32586.625413  1058.615650   3.917668   \n","1       2              1  1.000000e-10 -31189.621399  1053.414410   4.476038   \n","2       3              1  1.000000e-10 -29805.370984  1047.106412   4.916282   \n","3       4              1  1.000000e-10 -30412.817955  1044.818466   4.711976   \n","4       5              1  1.000000e-10 -31405.396078  1049.849450   3.797850   \n","..    ...            ...           ...           ...          ...        ...   \n","245     1             50  1.000000e+00 -32604.351129  1088.581253   2.884792   \n","246     2             50  1.000000e+00 -31242.094652  1089.915224   3.381021   \n","247     3             50  1.000000e+00 -29881.521004  1089.562456   3.758123   \n","248     4             50  1.000000e+00 -30471.671817  1086.952384   3.670738   \n","249     5             50  1.000000e+00 -31657.198702  1085.735722   2.747021   \n","\n","     num_baths  square_meters  year_built  num_crimes  ...  \\\n","0     7.126842     192.101034   -0.757726  -75.904084  ...   \n","1     7.996682     193.742519    0.812930  -75.533129  ...   \n","2     6.317971     188.796832   -1.523096  -76.388400  ...   \n","3     9.492917     189.692961   -0.766892  -76.860957  ...   \n","4     5.971341     189.547885   -0.095415  -75.176167  ...   \n","..         ...            ...         ...         ...  ...   \n","245   6.277713     191.155709   -0.000000  -75.716975  ...   \n","246   7.204187     192.503337    0.000000  -75.412434  ...   \n","247   5.507426     187.578657   -0.381861  -75.710976  ...   \n","248   8.564819     188.503710   -0.000000  -76.605951  ...   \n","249   5.059718     188.524121    0.000000  -74.973181  ...   \n","\n","     neighborhood_Sarrià-Sant Gervasi   floor_10    floor_2    floor_3  \\\n","0                           33.428130  44.272879  56.444276  57.885396   \n","1                           35.992292  36.003038  52.651858  66.409417   \n","2                           37.926515  45.501094  61.977180  66.666239   \n","3                           26.221743  38.361906  57.378565  60.068034   \n","4                           24.830778  33.015063  49.250481  64.616720   \n","..                                ...        ...        ...        ...   \n","245                         -0.000000   0.000000   9.460519  10.103815   \n","246                          0.000000   0.000000   9.331053  21.120634   \n","247                         -0.000000   0.000000  12.680365  17.250843   \n","248                         -0.000000   0.000000  14.390252  16.481862   \n","249                         -0.000000   0.000000  10.840507  24.078377   \n","\n","       floor_4    floor_5   floor_6   floor_7    floor_8   floor_9  \n","0    73.404529  76.642061  2.013203  3.776742  10.158322 -2.401222  \n","1    75.126816  74.616775  4.049733  6.425065   4.969810 -1.908622  \n","2    69.623761  78.723152  1.894611  6.227323   5.901172 -0.808511  \n","3    70.781013  75.278541  6.783332  5.619868   8.575298  2.863775  \n","4    62.177033  71.770241  2.165158  7.505489   8.875008  0.785281  \n","..         ...        ...       ...       ...        ...       ...  \n","245  25.372687  30.161026  0.000000  0.000000   5.730685 -0.000000  \n","246  29.966812  30.335900  0.000000  2.202022   0.801199 -0.000000  \n","247  20.419509  29.721570  0.000000  1.628526   1.271786 -0.000000  \n","248  27.912827  32.294465  1.952138  1.181696   4.143621  0.000000  \n","249  21.489972  31.763198  0.000000  3.060523   4.223587  0.000000  \n","\n","[250 rows x 35 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# results_v3_f6"]},{"cell_type":"markdown","id":"edf90886","metadata":{},"source":["## Function 7: function 6 but with Ridge"]},{"cell_type":"code","execution_count":27,"id":"d02e92fa","metadata":{},"outputs":[],"source":["def ridge_imputers(\n","        df: pd.DataFrame, numerical_features: list[str], boolean_features: list[str], \n","        categorical_features: list[str],target: str, n_folds: int = 5, \n","        use_imputers: bool = False, my_n_neighbors: int = 5, max_n_ridge_iter: int = 100\n","        ):\n","\n","    ###########################################################################\n","\n","    results_list = []  # To store all results we want for our data frame\n","    alpha_counter = 0  # Initialize a counter for the alphas\n","    features = numerical_features + boolean_features + categorical_features\n","\n","    ###########################################################################\n","\n","    # Create transformers for categorical and numerical features\n","    numerical_transformer = [('scaler', StandardScaler())]  # Standardize numerical features\n","        \n","    categorical_transformer = [('onehot', OneHotEncoder(handle_unknown='error', drop = 'first'))]  # One-hot encode categorical features, dropping first dummy\n","\n","    if use_imputers == True:\n","        # Add KNN imputer for numerical features. We append it, so that standardization is\n","        # performed before KNN Imputing\n","        numerical_transformer.append(('knn_imputer', KNNImputer(n_neighbors = my_n_neighbors)))\n","        # Optionally, use SimpleImputer for categorical features instead of KNN. We insert it \n","        # BEFORE one-hot encoding the categorical variables\n","        categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","\n","    # Set the boolean transformer based on use_imputers\n","    if use_imputers == True:\n","        # Optionally, use SimpleImputer for boolean features\n","        boolean_transformer = [('simple_imputer', SimpleImputer(strategy='most_frequent'))]\n","    else:\n","        # Pass through boolean features without transformation\n","        boolean_transformer = [('passthrough', 'passthrough')]\n","\n","    # Create the ColumnTransformer to apply different transformations to numerical\n","    # columns, categorical and dummy (boolean) columns\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', Pipeline(numerical_transformer), numerical_features),\n","            ('cat', Pipeline(categorical_transformer), categorical_features),\n","            ('bool', Pipeline(boolean_transformer), boolean_features)\n","        ]\n","    )\n","\n","    # Create steps of the pipeline, which begins with the preprocessor. It will\n","    # apply the transformation and imputations (if use_imputers is set to True) \n","    # to the different features in the order specified in ColumnTransformer\n","    steps = [('preprocessor', preprocessor)]\n","    \n","    ###########################################################################\n","\n","    # Now, we create a loop that iterates over different values of the alpha\n","    # parameter. We generate 50 alphas\n","    for alpha in np.logspace(-10, 0, 50):\n","    \n","        # We add 1 to the counter of alpha each time we consider another alpha\n","        alpha_counter += 1\n","\n","        # We add Ridge to the pipeline, with a specific alpha\n","        steps.append(('ridge', \n","                    Ridge(alpha = alpha,\n","                          fit_intercept = True,\n","                          max_iter = max_n_ridge_iter\n","                        )))\n","        \n","        # We create the pipeline with the steps specified above\n","        pipeline = Pipeline(steps)\n","\n","        # Perform cross-validation: on each fold, it will scale and fit the training\n","        # data, and compute the negative mean squared error (so a higher value, i.e.,\n","        # more negative, is better).\n","\n","        cv_results = cross_validate(\n","            estimator = pipeline, \n","            X = df[features], \n","            y = df[target], cv = n_folds, scoring = 'neg_mean_squared_error',\n","            return_estimator = True\n","            )\n","\n","        neg_mse = cv_results['test_score']  # Negative MSE scores\n","        estimators = cv_results['estimator']  # List of fitted estimators for each fold\n","        \n","        # Loop through each fold result to store the information\n","        for index, (score, estimator) in enumerate(zip(neg_mse, estimators)):\n","            # We access the information associated to the ridge\n","            ridge = estimator.named_steps['ridge']\n","            # We store the coefficients estimated by ridge\n","            coefs = ridge.coef_\n","            # We store the intercept estimated by ridge\n","            intercept = ridge.intercept_\n","\n","            # Create a dictionary to store the results for this fold\n","            result_dict = {\n","                'Fold': index + 1,\n","                'Alpha_counter': alpha_counter,\n","                'Alpha': alpha,\n","                'Negative MSE': score,\n","                'Intercept': intercept\n","            }\n","\n","            # Get feature names after one-hot encoding\n","            cat_feature_names = estimator.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n","            combined_features = np.concatenate([numerical_features, boolean_features, cat_feature_names])\n","\n","            # Assign coefficients to the corresponding features\n","            for i, feature in enumerate(combined_features):\n","                result_dict[feature] = coefs[i]\n","\n","            # Append the result to the list\n","            results_list.append(result_dict)\n","\n","        # Lastly, we remove the last step of the pipeline so that the \n","        # Ridge step does not accumulate with different hyperparameters.\n","        # We do this for each iteration of alpha\n","        steps.pop()\n","\n","    # Convert the results list to a DataFrame\n","    results_df = pd.DataFrame(results_list)\n","\n","    return results_df"]},{"cell_type":"markdown","id":"68bf9f2d","metadata":{},"source":["### Training with function 7"]},{"cell_type":"code","execution_count":28,"id":"20ae79e8","metadata":{},"outputs":[],"source":["# results_v3_f7 = ridge_imputers(\n","    # df = df_v3, numerical_features = numerical_cols_v3, boolean_features = boolean_cols_v3, \n","    # categorical_features = categorical_cols_v3, target = 'price', n_folds = 5, use_imputers = True, my_n_neighbors = 5, \n","    # max_n_ridge_iter = 500)\n","\n","# results_v3_f7.to_csv('3_results_datasets/results_v3_f7.csv')"]},{"cell_type":"code","execution_count":29,"id":"5c663737","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fold</th>\n","      <th>Alpha_counter</th>\n","      <th>Alpha</th>\n","      <th>Negative MSE</th>\n","      <th>Intercept</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>num_crimes</th>\n","      <th>...</th>\n","      <th>neighborhood_Sarrià-Sant Gervasi</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-32586.625413</td>\n","      <td>1058.615650</td>\n","      <td>3.917668</td>\n","      <td>7.126842</td>\n","      <td>192.101034</td>\n","      <td>-0.757726</td>\n","      <td>-75.904084</td>\n","      <td>...</td>\n","      <td>33.428130</td>\n","      <td>44.272879</td>\n","      <td>56.444276</td>\n","      <td>57.885396</td>\n","      <td>73.404529</td>\n","      <td>76.642061</td>\n","      <td>2.013203</td>\n","      <td>3.776742</td>\n","      <td>10.158322</td>\n","      <td>-2.401222</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-31189.621399</td>\n","      <td>1053.414410</td>\n","      <td>4.476038</td>\n","      <td>7.996682</td>\n","      <td>193.742519</td>\n","      <td>0.812930</td>\n","      <td>-75.533129</td>\n","      <td>...</td>\n","      <td>35.992292</td>\n","      <td>36.003038</td>\n","      <td>52.651858</td>\n","      <td>66.409417</td>\n","      <td>75.126816</td>\n","      <td>74.616775</td>\n","      <td>4.049733</td>\n","      <td>6.425065</td>\n","      <td>4.969810</td>\n","      <td>-1.908622</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-29805.370984</td>\n","      <td>1047.106412</td>\n","      <td>4.916282</td>\n","      <td>6.317971</td>\n","      <td>188.796832</td>\n","      <td>-1.523096</td>\n","      <td>-76.388400</td>\n","      <td>...</td>\n","      <td>37.926515</td>\n","      <td>45.501094</td>\n","      <td>61.977180</td>\n","      <td>66.666239</td>\n","      <td>69.623761</td>\n","      <td>78.723152</td>\n","      <td>1.894611</td>\n","      <td>6.227323</td>\n","      <td>5.901172</td>\n","      <td>-0.808511</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-30412.817955</td>\n","      <td>1044.818466</td>\n","      <td>4.711976</td>\n","      <td>9.492917</td>\n","      <td>189.692961</td>\n","      <td>-0.766892</td>\n","      <td>-76.860957</td>\n","      <td>...</td>\n","      <td>26.221743</td>\n","      <td>38.361906</td>\n","      <td>57.378565</td>\n","      <td>60.068034</td>\n","      <td>70.781013</td>\n","      <td>75.278541</td>\n","      <td>6.783332</td>\n","      <td>5.619868</td>\n","      <td>8.575298</td>\n","      <td>2.863775</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1.000000e-10</td>\n","      <td>-31405.396078</td>\n","      <td>1049.849450</td>\n","      <td>3.797850</td>\n","      <td>5.971341</td>\n","      <td>189.547885</td>\n","      <td>-0.095415</td>\n","      <td>-75.176167</td>\n","      <td>...</td>\n","      <td>24.830778</td>\n","      <td>33.015063</td>\n","      <td>49.250481</td>\n","      <td>64.616720</td>\n","      <td>62.177033</td>\n","      <td>71.770241</td>\n","      <td>2.165158</td>\n","      <td>7.505489</td>\n","      <td>8.875008</td>\n","      <td>0.785281</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>245</th>\n","      <td>1</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-32584.184696</td>\n","      <td>1059.284122</td>\n","      <td>3.916763</td>\n","      <td>7.128586</td>\n","      <td>192.073894</td>\n","      <td>-0.756566</td>\n","      <td>-75.891041</td>\n","      <td>...</td>\n","      <td>32.634630</td>\n","      <td>43.477891</td>\n","      <td>55.621420</td>\n","      <td>57.057030</td>\n","      <td>72.547391</td>\n","      <td>75.790454</td>\n","      <td>2.007384</td>\n","      <td>3.766623</td>\n","      <td>10.142581</td>\n","      <td>-2.414504</td>\n","    </tr>\n","    <tr>\n","      <th>246</th>\n","      <td>2</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-31187.627056</td>\n","      <td>1054.139730</td>\n","      <td>4.476951</td>\n","      <td>7.997505</td>\n","      <td>193.708590</td>\n","      <td>0.814695</td>\n","      <td>-75.521024</td>\n","      <td>...</td>\n","      <td>35.192077</td>\n","      <td>35.203529</td>\n","      <td>51.839256</td>\n","      <td>65.561366</td>\n","      <td>74.259067</td>\n","      <td>73.760643</td>\n","      <td>4.033803</td>\n","      <td>6.417302</td>\n","      <td>4.964618</td>\n","      <td>-1.913205</td>\n","    </tr>\n","    <tr>\n","      <th>247</th>\n","      <td>3</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-29804.112576</td>\n","      <td>1047.868965</td>\n","      <td>4.916530</td>\n","      <td>6.319826</td>\n","      <td>188.764847</td>\n","      <td>-1.520350</td>\n","      <td>-76.375991</td>\n","      <td>...</td>\n","      <td>37.112792</td>\n","      <td>44.690726</td>\n","      <td>61.128867</td>\n","      <td>65.811470</td>\n","      <td>68.759782</td>\n","      <td>77.851944</td>\n","      <td>1.885387</td>\n","      <td>6.210743</td>\n","      <td>5.884575</td>\n","      <td>-0.818417</td>\n","    </tr>\n","    <tr>\n","      <th>248</th>\n","      <td>4</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-30413.765746</td>\n","      <td>1045.545511</td>\n","      <td>4.711312</td>\n","      <td>9.492476</td>\n","      <td>189.660485</td>\n","      <td>-0.765057</td>\n","      <td>-76.848688</td>\n","      <td>...</td>\n","      <td>25.487581</td>\n","      <td>37.617542</td>\n","      <td>56.597949</td>\n","      <td>59.279474</td>\n","      <td>69.975081</td>\n","      <td>74.468273</td>\n","      <td>6.770129</td>\n","      <td>5.608413</td>\n","      <td>8.560664</td>\n","      <td>2.856791</td>\n","    </tr>\n","    <tr>\n","      <th>249</th>\n","      <td>5</td>\n","      <td>50</td>\n","      <td>1.000000e+00</td>\n","      <td>-31408.191880</td>\n","      <td>1050.519095</td>\n","      <td>3.797622</td>\n","      <td>5.971956</td>\n","      <td>189.517791</td>\n","      <td>-0.093532</td>\n","      <td>-75.164653</td>\n","      <td>...</td>\n","      <td>24.148622</td>\n","      <td>32.320110</td>\n","      <td>48.544033</td>\n","      <td>63.871511</td>\n","      <td>61.432145</td>\n","      <td>71.014211</td>\n","      <td>2.153554</td>\n","      <td>7.493606</td>\n","      <td>8.859945</td>\n","      <td>0.777938</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>250 rows × 35 columns</p>\n","</div>"],"text/plain":["     Fold  Alpha_counter         Alpha  Negative MSE    Intercept  num_rooms  \\\n","0       1              1  1.000000e-10 -32586.625413  1058.615650   3.917668   \n","1       2              1  1.000000e-10 -31189.621399  1053.414410   4.476038   \n","2       3              1  1.000000e-10 -29805.370984  1047.106412   4.916282   \n","3       4              1  1.000000e-10 -30412.817955  1044.818466   4.711976   \n","4       5              1  1.000000e-10 -31405.396078  1049.849450   3.797850   \n","..    ...            ...           ...           ...          ...        ...   \n","245     1             50  1.000000e+00 -32584.184696  1059.284122   3.916763   \n","246     2             50  1.000000e+00 -31187.627056  1054.139730   4.476951   \n","247     3             50  1.000000e+00 -29804.112576  1047.868965   4.916530   \n","248     4             50  1.000000e+00 -30413.765746  1045.545511   4.711312   \n","249     5             50  1.000000e+00 -31408.191880  1050.519095   3.797622   \n","\n","     num_baths  square_meters  year_built  num_crimes  ...  \\\n","0     7.126842     192.101034   -0.757726  -75.904084  ...   \n","1     7.996682     193.742519    0.812930  -75.533129  ...   \n","2     6.317971     188.796832   -1.523096  -76.388400  ...   \n","3     9.492917     189.692961   -0.766892  -76.860957  ...   \n","4     5.971341     189.547885   -0.095415  -75.176167  ...   \n","..         ...            ...         ...         ...  ...   \n","245   7.128586     192.073894   -0.756566  -75.891041  ...   \n","246   7.997505     193.708590    0.814695  -75.521024  ...   \n","247   6.319826     188.764847   -1.520350  -76.375991  ...   \n","248   9.492476     189.660485   -0.765057  -76.848688  ...   \n","249   5.971956     189.517791   -0.093532  -75.164653  ...   \n","\n","     neighborhood_Sarrià-Sant Gervasi   floor_10    floor_2    floor_3  \\\n","0                           33.428130  44.272879  56.444276  57.885396   \n","1                           35.992292  36.003038  52.651858  66.409417   \n","2                           37.926515  45.501094  61.977180  66.666239   \n","3                           26.221743  38.361906  57.378565  60.068034   \n","4                           24.830778  33.015063  49.250481  64.616720   \n","..                                ...        ...        ...        ...   \n","245                         32.634630  43.477891  55.621420  57.057030   \n","246                         35.192077  35.203529  51.839256  65.561366   \n","247                         37.112792  44.690726  61.128867  65.811470   \n","248                         25.487581  37.617542  56.597949  59.279474   \n","249                         24.148622  32.320110  48.544033  63.871511   \n","\n","       floor_4    floor_5   floor_6   floor_7    floor_8   floor_9  \n","0    73.404529  76.642061  2.013203  3.776742  10.158322 -2.401222  \n","1    75.126816  74.616775  4.049733  6.425065   4.969810 -1.908622  \n","2    69.623761  78.723152  1.894611  6.227323   5.901172 -0.808511  \n","3    70.781013  75.278541  6.783332  5.619868   8.575298  2.863775  \n","4    62.177033  71.770241  2.165158  7.505489   8.875008  0.785281  \n","..         ...        ...       ...       ...        ...       ...  \n","245  72.547391  75.790454  2.007384  3.766623  10.142581 -2.414504  \n","246  74.259067  73.760643  4.033803  6.417302   4.964618 -1.913205  \n","247  68.759782  77.851944  1.885387  6.210743   5.884575 -0.818417  \n","248  69.975081  74.468273  6.770129  5.608413   8.560664  2.856791  \n","249  61.432145  71.014211  2.153554  7.493606   8.859945  0.777938  \n","\n","[250 rows x 35 columns]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# results_v3_f7"]},{"cell_type":"markdown","id":"06b35300","metadata":{},"source":["## Function 8: function 5 but with imputation of median for numerical variables"]},{"cell_type":"code","execution_count":9,"id":"c96c225f","metadata":{},"outputs":[],"source":["def lin_reg_with_preprocessing_combos_median_imputing(\n","    df: pd.DataFrame, numerical_features: list[str], boolean_features: list[str], \n","    categorical_features: list[str], target: str, n_folds: int = 5, use_imputers: bool = False,\n","):\n","\n","    def flatten_combo(combo):\n","        \"\"\"\n","        Flattens the combination by including all variables from nested lists\n","        when one element of a list is selected.\n","        \"\"\"\n","        return list(itertools.chain(*[x if isinstance(x, list) else [x] for x in combo]))\n","\n","    results_list = []  # To store all results we want for our data frame\n","    combo_counter = 0  # Initialize a counter for the combinations\n","    features = numerical_features + boolean_features + categorical_features\n","\n","    for r in range(1, len(features) + 1):\n","        for combo in itertools.combinations(features, r):\n","            \n","            combo_counter += 1\n","\n","            # We flatten the list of the resulting combination of features\n","            flat_combo = flatten_combo(combo)\n","\n","            # Determine which types of features are present in the current combination\n","            current_numerical_features = [f for f in flat_combo if f in numerical_features]\n","            current_boolean_features = [f for f in flat_combo if f in boolean_features]\n","            current_categorical_features = [f for f in flat_combo if f in categorical_features]\n","\n","            # Build the transformers dynamically based on the present feature types\n","            transformers = []\n","            \n","            # Add numerical transformer if numerical features are present\n","            if current_numerical_features:\n","                numerical_transformer = [('scaler', StandardScaler())]\n","                if use_imputers:\n","                    numerical_transformer.append(('median_imputer', SimpleImputer(strategy='median')))\n","                transformers.append(('num', Pipeline(numerical_transformer), current_numerical_features))\n","                \n","            # Add categorical transformer if categorical features are present\n","            if current_categorical_features:\n","                categorical_transformer = [('onehot', OneHotEncoder(handle_unknown='error', drop='first'))]\n","                if use_imputers:\n","                    categorical_transformer.insert(0, ('simple_imputer', SimpleImputer(strategy='most_frequent')))\n","                transformers.append(('cat', Pipeline(categorical_transformer), current_categorical_features))\n","                \n","            # Add boolean transformer if boolean features are present\n","            if current_boolean_features:\n","                if use_imputers:\n","                    boolean_transformer = [('simple_imputer', SimpleImputer(strategy='most_frequent'))]\n","                else:\n","                    boolean_transformer = [('passthrough', 'passthrough')]\n","                transformers.append(('bool', Pipeline(boolean_transformer), current_boolean_features))\n","                \n","            # Define the preprocessor with only the applicable transformers\n","            preprocessor = ColumnTransformer(transformers=transformers)\n","\n","            # Pipeline: apply transformations and linear regression\n","            pipeline = Pipeline([\n","                ('preprocessor', preprocessor),\n","                ('linreg', LinearRegression(fit_intercept=True))\n","            ])\n","\n","            # Perform cross-validation and get negative MSE scores\n","            cv_results = cross_validate(\n","                estimator=pipeline,\n","                X=df[flat_combo],\n","                y=df[target],\n","                cv=n_folds,\n","                scoring='neg_mean_squared_error',\n","                return_estimator=True\n","            )\n","\n","            neg_mse = cv_results['test_score']\n","            estimators = cv_results['estimator']\n","\n","            # Store results for each fold\n","            for index, (score, estimator) in enumerate(zip(neg_mse, estimators)):\n","                linreg = estimator.named_steps['linreg']\n","                coefs = linreg.coef_\n","                intercept = linreg.intercept_\n","\n","                result_dict = {\n","                    'Fold': index + 1,\n","                    'Combination ID': combo_counter,\n","                    'Negative MSE': score,\n","                    'Intercept': intercept\n","                }\n","\n","                # Get feature names post one-hot encoding and exclude original categorical columns\n","                if current_categorical_features:\n","                    cat_feature_names = estimator.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(current_categorical_features)\n","                else:\n","                    cat_feature_names = []\n","\n","                # Combine the feature names, excluding the original categorical columns\n","                combined_features = np.concatenate([current_numerical_features, current_boolean_features, cat_feature_names])\n","\n","                # Assign coefficients to the corresponding features\n","                for i, feature in enumerate(combined_features):\n","                    result_dict[feature] = coefs[i]\n","\n","                # Assign NaN for features not in the current combination, excluding original categorical columns\n","                for feature in features:\n","                    if feature in numerical_features or feature in boolean_features:\n","                        if feature not in combined_features:\n","                            result_dict[feature] = float('nan')\n","\n","                results_list.append(result_dict)\n","\n","    # Convert results to DataFrame\n","    results_df = pd.DataFrame(results_list)\n","    return results_df"]},{"cell_type":"markdown","id":"6db216f5","metadata":{},"source":["### Training with function 8"]},{"cell_type":"markdown","id":"9d231b23","metadata":{},"source":["#### Training with dataset v3"]},{"cell_type":"code","execution_count":59,"id":"8b42e0b2","metadata":{},"outputs":[],"source":["# results_v3_f8 = lin_reg_with_preprocessing_combos_median_imputing(\n","#     df = df_v3, numerical_features = numerical_cols_v3, boolean_features = boolean_cols_v3,\n","#     categorical_features = categorical_cols_v3, target = 'price', n_folds = 5,\n","#     use_imputers = True\n","# )\n","\n","# results_v3_f8.to_csv('3_results_datasets/results_v3_f8.csv')"]},{"cell_type":"markdown","id":"6612b4d8","metadata":{},"source":["The computation has lasted for approx. 6 minutes."]},{"cell_type":"code","execution_count":60,"id":"c699e7a8","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fold</th>\n","      <th>Combination ID</th>\n","      <th>Negative MSE</th>\n","      <th>Intercept</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>num_crimes</th>\n","      <th>is_furnished</th>\n","      <th>...</th>\n","      <th>neighborhood_Sarrià-Sant Gervasi</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>-72989.051984</td>\n","      <td>1099.686250</td>\n","      <td>5.927694</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>-70145.790556</td>\n","      <td>1101.458125</td>\n","      <td>11.108159</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>-75719.267630</td>\n","      <td>1097.982500</td>\n","      <td>7.062621</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>-74018.007181</td>\n","      <td>1099.884531</td>\n","      <td>9.092078</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>-75968.024362</td>\n","      <td>1099.069844</td>\n","      <td>8.212737</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20470</th>\n","      <td>1</td>\n","      <td>4095</td>\n","      <td>-32452.415571</td>\n","      <td>1057.070134</td>\n","      <td>3.976782</td>\n","      <td>6.839269</td>\n","      <td>192.657693</td>\n","      <td>-0.978987</td>\n","      <td>-76.022831</td>\n","      <td>-8.088217</td>\n","      <td>...</td>\n","      <td>33.492579</td>\n","      <td>44.181298</td>\n","      <td>56.887882</td>\n","      <td>58.365522</td>\n","      <td>72.921340</td>\n","      <td>76.884128</td>\n","      <td>2.398569</td>\n","      <td>4.064126</td>\n","      <td>10.355716</td>\n","      <td>-2.251321</td>\n","    </tr>\n","    <tr>\n","      <th>20471</th>\n","      <td>2</td>\n","      <td>4095</td>\n","      <td>-31193.011222</td>\n","      <td>1050.787805</td>\n","      <td>4.288562</td>\n","      <td>7.931738</td>\n","      <td>194.671489</td>\n","      <td>0.483221</td>\n","      <td>-75.230631</td>\n","      <td>-1.776336</td>\n","      <td>...</td>\n","      <td>36.646567</td>\n","      <td>36.163698</td>\n","      <td>53.583105</td>\n","      <td>67.391105</td>\n","      <td>75.046845</td>\n","      <td>74.811854</td>\n","      <td>4.232608</td>\n","      <td>6.053883</td>\n","      <td>5.384287</td>\n","      <td>-1.848843</td>\n","    </tr>\n","    <tr>\n","      <th>20472</th>\n","      <td>3</td>\n","      <td>4095</td>\n","      <td>-29785.538590</td>\n","      <td>1046.218061</td>\n","      <td>4.733031</td>\n","      <td>6.125433</td>\n","      <td>189.445293</td>\n","      <td>-1.756328</td>\n","      <td>-76.351799</td>\n","      <td>-1.042517</td>\n","      <td>...</td>\n","      <td>37.915586</td>\n","      <td>44.888069</td>\n","      <td>62.918897</td>\n","      <td>67.505622</td>\n","      <td>69.336506</td>\n","      <td>78.994132</td>\n","      <td>1.945725</td>\n","      <td>6.003837</td>\n","      <td>6.250057</td>\n","      <td>-0.997016</td>\n","    </tr>\n","    <tr>\n","      <th>20473</th>\n","      <td>4</td>\n","      <td>4095</td>\n","      <td>-30364.515882</td>\n","      <td>1043.114206</td>\n","      <td>4.717118</td>\n","      <td>9.489858</td>\n","      <td>190.171410</td>\n","      <td>-0.938384</td>\n","      <td>-76.809406</td>\n","      <td>1.862910</td>\n","      <td>...</td>\n","      <td>26.929488</td>\n","      <td>38.164357</td>\n","      <td>58.218733</td>\n","      <td>60.672196</td>\n","      <td>71.115293</td>\n","      <td>76.292979</td>\n","      <td>6.746641</td>\n","      <td>5.543317</td>\n","      <td>8.469710</td>\n","      <td>3.092547</td>\n","    </tr>\n","    <tr>\n","      <th>20474</th>\n","      <td>5</td>\n","      <td>4095</td>\n","      <td>-31213.207884</td>\n","      <td>1048.349432</td>\n","      <td>3.793155</td>\n","      <td>6.111537</td>\n","      <td>189.929255</td>\n","      <td>-0.093751</td>\n","      <td>-74.999157</td>\n","      <td>-2.876432</td>\n","      <td>...</td>\n","      <td>24.819878</td>\n","      <td>31.951723</td>\n","      <td>49.699456</td>\n","      <td>64.527451</td>\n","      <td>62.158663</td>\n","      <td>71.225182</td>\n","      <td>2.175615</td>\n","      <td>7.534132</td>\n","      <td>9.338586</td>\n","      <td>1.040406</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20475 rows × 34 columns</p>\n","</div>"],"text/plain":["       Fold  Combination ID  Negative MSE    Intercept  num_rooms  num_baths  \\\n","0         1               1 -72989.051984  1099.686250   5.927694        NaN   \n","1         2               1 -70145.790556  1101.458125  11.108159        NaN   \n","2         3               1 -75719.267630  1097.982500   7.062621        NaN   \n","3         4               1 -74018.007181  1099.884531   9.092078        NaN   \n","4         5               1 -75968.024362  1099.069844   8.212737        NaN   \n","...     ...             ...           ...          ...        ...        ...   \n","20470     1            4095 -32452.415571  1057.070134   3.976782   6.839269   \n","20471     2            4095 -31193.011222  1050.787805   4.288562   7.931738   \n","20472     3            4095 -29785.538590  1046.218061   4.733031   6.125433   \n","20473     4            4095 -30364.515882  1043.114206   4.717118   9.489858   \n","20474     5            4095 -31213.207884  1048.349432   3.793155   6.111537   \n","\n","       square_meters  year_built  num_crimes  is_furnished  ...  \\\n","0                NaN         NaN         NaN           NaN  ...   \n","1                NaN         NaN         NaN           NaN  ...   \n","2                NaN         NaN         NaN           NaN  ...   \n","3                NaN         NaN         NaN           NaN  ...   \n","4                NaN         NaN         NaN           NaN  ...   \n","...              ...         ...         ...           ...  ...   \n","20470     192.657693   -0.978987  -76.022831     -8.088217  ...   \n","20471     194.671489    0.483221  -75.230631     -1.776336  ...   \n","20472     189.445293   -1.756328  -76.351799     -1.042517  ...   \n","20473     190.171410   -0.938384  -76.809406      1.862910  ...   \n","20474     189.929255   -0.093751  -74.999157     -2.876432  ...   \n","\n","       neighborhood_Sarrià-Sant Gervasi   floor_10    floor_2    floor_3  \\\n","0                                   NaN        NaN        NaN        NaN   \n","1                                   NaN        NaN        NaN        NaN   \n","2                                   NaN        NaN        NaN        NaN   \n","3                                   NaN        NaN        NaN        NaN   \n","4                                   NaN        NaN        NaN        NaN   \n","...                                 ...        ...        ...        ...   \n","20470                         33.492579  44.181298  56.887882  58.365522   \n","20471                         36.646567  36.163698  53.583105  67.391105   \n","20472                         37.915586  44.888069  62.918897  67.505622   \n","20473                         26.929488  38.164357  58.218733  60.672196   \n","20474                         24.819878  31.951723  49.699456  64.527451   \n","\n","         floor_4    floor_5   floor_6   floor_7    floor_8   floor_9  \n","0            NaN        NaN       NaN       NaN        NaN       NaN  \n","1            NaN        NaN       NaN       NaN        NaN       NaN  \n","2            NaN        NaN       NaN       NaN        NaN       NaN  \n","3            NaN        NaN       NaN       NaN        NaN       NaN  \n","4            NaN        NaN       NaN       NaN        NaN       NaN  \n","...          ...        ...       ...       ...        ...       ...  \n","20470  72.921340  76.884128  2.398569  4.064126  10.355716 -2.251321  \n","20471  75.046845  74.811854  4.232608  6.053883   5.384287 -1.848843  \n","20472  69.336506  78.994132  1.945725  6.003837   6.250057 -0.997016  \n","20473  71.115293  76.292979  6.746641  5.543317   8.469710  3.092547  \n","20474  62.158663  71.225182  2.175615  7.534132   9.338586  1.040406  \n","\n","[20475 rows x 34 columns]"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# results_v3_f8"]},{"cell_type":"markdown","id":"6a387791","metadata":{},"source":["#### Training with dataset v4 (outliers of `num_rooms` treated)"]},{"cell_type":"code","execution_count":10,"id":"fd7f12d8","metadata":{},"outputs":[],"source":["# results_v4_f8 = lin_reg_with_preprocessing_combos_median_imputing(\n","#     df = df_v4, numerical_features = numerical_cols_v3, boolean_features = boolean_cols_v3,\n","#     categorical_features = categorical_cols_v3, target = 'price', n_folds = 5,\n","#     use_imputers = True\n","# )\n","\n","# results_v4_f8.to_csv('3_results_datasets/results_v4_f8.csv')"]},{"cell_type":"markdown","id":"991bf982","metadata":{},"source":["Lasted for 5 mins and 20 seconds."]},{"cell_type":"code","execution_count":11,"id":"97ff9465","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fold</th>\n","      <th>Combination ID</th>\n","      <th>Negative MSE</th>\n","      <th>Intercept</th>\n","      <th>num_rooms</th>\n","      <th>num_baths</th>\n","      <th>square_meters</th>\n","      <th>year_built</th>\n","      <th>num_crimes</th>\n","      <th>is_furnished</th>\n","      <th>...</th>\n","      <th>neighborhood_Sarrià-Sant Gervasi</th>\n","      <th>floor_10</th>\n","      <th>floor_2</th>\n","      <th>floor_3</th>\n","      <th>floor_4</th>\n","      <th>floor_5</th>\n","      <th>floor_6</th>\n","      <th>floor_7</th>\n","      <th>floor_8</th>\n","      <th>floor_9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>-72645.582001</td>\n","      <td>1099.686250</td>\n","      <td>17.798024</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>-69985.372982</td>\n","      <td>1101.458125</td>\n","      <td>21.319870</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>-75476.784039</td>\n","      <td>1097.982500</td>\n","      <td>18.969422</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>-73962.103178</td>\n","      <td>1099.884531</td>\n","      <td>20.824257</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>-75347.649281</td>\n","      <td>1099.069844</td>\n","      <td>16.536627</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20470</th>\n","      <td>1</td>\n","      <td>4095</td>\n","      <td>-32107.578137</td>\n","      <td>1058.344885</td>\n","      <td>15.600898</td>\n","      <td>6.736999</td>\n","      <td>192.494831</td>\n","      <td>-0.880372</td>\n","      <td>-76.061736</td>\n","      <td>-9.137359</td>\n","      <td>...</td>\n","      <td>33.827971</td>\n","      <td>44.106428</td>\n","      <td>56.952129</td>\n","      <td>58.378583</td>\n","      <td>72.390446</td>\n","      <td>76.886940</td>\n","      <td>2.107064</td>\n","      <td>3.906826</td>\n","      <td>10.273763</td>\n","      <td>-2.538623</td>\n","    </tr>\n","    <tr>\n","      <th>20471</th>\n","      <td>2</td>\n","      <td>4095</td>\n","      <td>-31087.107816</td>\n","      <td>1052.209791</td>\n","      <td>17.346073</td>\n","      <td>7.846080</td>\n","      <td>194.500189</td>\n","      <td>0.363171</td>\n","      <td>-74.988043</td>\n","      <td>-3.000922</td>\n","      <td>...</td>\n","      <td>36.868358</td>\n","      <td>37.612424</td>\n","      <td>53.104171</td>\n","      <td>67.612733</td>\n","      <td>76.192771</td>\n","      <td>75.012855</td>\n","      <td>4.304377</td>\n","      <td>5.682957</td>\n","      <td>5.336745</td>\n","      <td>-1.933506</td>\n","    </tr>\n","    <tr>\n","      <th>20472</th>\n","      <td>3</td>\n","      <td>4095</td>\n","      <td>-29542.245272</td>\n","      <td>1046.984267</td>\n","      <td>16.520627</td>\n","      <td>6.027764</td>\n","      <td>189.327441</td>\n","      <td>-1.971938</td>\n","      <td>-76.227249</td>\n","      <td>-2.032880</td>\n","      <td>...</td>\n","      <td>39.189093</td>\n","      <td>45.999628</td>\n","      <td>63.210959</td>\n","      <td>67.785116</td>\n","      <td>70.235657</td>\n","      <td>79.155097</td>\n","      <td>1.647823</td>\n","      <td>5.301485</td>\n","      <td>6.397510</td>\n","      <td>-0.536725</td>\n","    </tr>\n","    <tr>\n","      <th>20473</th>\n","      <td>4</td>\n","      <td>4095</td>\n","      <td>-30124.080916</td>\n","      <td>1043.414813</td>\n","      <td>16.584430</td>\n","      <td>9.624262</td>\n","      <td>189.962911</td>\n","      <td>-1.355500</td>\n","      <td>-76.526701</td>\n","      <td>0.895805</td>\n","      <td>...</td>\n","      <td>28.268473</td>\n","      <td>39.267347</td>\n","      <td>59.210079</td>\n","      <td>62.230175</td>\n","      <td>72.209402</td>\n","      <td>76.916889</td>\n","      <td>6.449547</td>\n","      <td>5.075136</td>\n","      <td>8.232104</td>\n","      <td>2.946468</td>\n","    </tr>\n","    <tr>\n","      <th>20474</th>\n","      <td>5</td>\n","      <td>4095</td>\n","      <td>-30932.159180</td>\n","      <td>1049.435923</td>\n","      <td>16.043982</td>\n","      <td>6.117468</td>\n","      <td>189.857858</td>\n","      <td>-0.332407</td>\n","      <td>-75.110742</td>\n","      <td>-3.857981</td>\n","      <td>...</td>\n","      <td>26.424845</td>\n","      <td>32.908437</td>\n","      <td>49.728541</td>\n","      <td>65.587911</td>\n","      <td>63.142343</td>\n","      <td>71.796593</td>\n","      <td>1.706518</td>\n","      <td>6.970798</td>\n","      <td>9.124279</td>\n","      <td>0.814996</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20475 rows × 34 columns</p>\n","</div>"],"text/plain":["       Fold  Combination ID  Negative MSE    Intercept  num_rooms  num_baths  \\\n","0         1               1 -72645.582001  1099.686250  17.798024        NaN   \n","1         2               1 -69985.372982  1101.458125  21.319870        NaN   \n","2         3               1 -75476.784039  1097.982500  18.969422        NaN   \n","3         4               1 -73962.103178  1099.884531  20.824257        NaN   \n","4         5               1 -75347.649281  1099.069844  16.536627        NaN   \n","...     ...             ...           ...          ...        ...        ...   \n","20470     1            4095 -32107.578137  1058.344885  15.600898   6.736999   \n","20471     2            4095 -31087.107816  1052.209791  17.346073   7.846080   \n","20472     3            4095 -29542.245272  1046.984267  16.520627   6.027764   \n","20473     4            4095 -30124.080916  1043.414813  16.584430   9.624262   \n","20474     5            4095 -30932.159180  1049.435923  16.043982   6.117468   \n","\n","       square_meters  year_built  num_crimes  is_furnished  ...  \\\n","0                NaN         NaN         NaN           NaN  ...   \n","1                NaN         NaN         NaN           NaN  ...   \n","2                NaN         NaN         NaN           NaN  ...   \n","3                NaN         NaN         NaN           NaN  ...   \n","4                NaN         NaN         NaN           NaN  ...   \n","...              ...         ...         ...           ...  ...   \n","20470     192.494831   -0.880372  -76.061736     -9.137359  ...   \n","20471     194.500189    0.363171  -74.988043     -3.000922  ...   \n","20472     189.327441   -1.971938  -76.227249     -2.032880  ...   \n","20473     189.962911   -1.355500  -76.526701      0.895805  ...   \n","20474     189.857858   -0.332407  -75.110742     -3.857981  ...   \n","\n","       neighborhood_Sarrià-Sant Gervasi   floor_10    floor_2    floor_3  \\\n","0                                   NaN        NaN        NaN        NaN   \n","1                                   NaN        NaN        NaN        NaN   \n","2                                   NaN        NaN        NaN        NaN   \n","3                                   NaN        NaN        NaN        NaN   \n","4                                   NaN        NaN        NaN        NaN   \n","...                                 ...        ...        ...        ...   \n","20470                         33.827971  44.106428  56.952129  58.378583   \n","20471                         36.868358  37.612424  53.104171  67.612733   \n","20472                         39.189093  45.999628  63.210959  67.785116   \n","20473                         28.268473  39.267347  59.210079  62.230175   \n","20474                         26.424845  32.908437  49.728541  65.587911   \n","\n","         floor_4    floor_5   floor_6   floor_7    floor_8   floor_9  \n","0            NaN        NaN       NaN       NaN        NaN       NaN  \n","1            NaN        NaN       NaN       NaN        NaN       NaN  \n","2            NaN        NaN       NaN       NaN        NaN       NaN  \n","3            NaN        NaN       NaN       NaN        NaN       NaN  \n","4            NaN        NaN       NaN       NaN        NaN       NaN  \n","...          ...        ...       ...       ...        ...       ...  \n","20470  72.390446  76.886940  2.107064  3.906826  10.273763 -2.538623  \n","20471  76.192771  75.012855  4.304377  5.682957   5.336745 -1.933506  \n","20472  70.235657  79.155097  1.647823  5.301485   6.397510 -0.536725  \n","20473  72.209402  76.916889  6.449547  5.075136   8.232104  2.946468  \n","20474  63.142343  71.796593  1.706518  6.970798   9.124279  0.814996  \n","\n","[20475 rows x 34 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# results_v4_f8"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"py312","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}
